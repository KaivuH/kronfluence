Query Sequence:
Prompt: Using a distributed database has many advantages.; Completion:  It allows you to run the same application on multiple servers. This makes it easier to scale your application. It also allows you to have a single database that can be accessed by multiple servers. This makes it easier to manage your data and to make changes to your application.

Top Influential Sequences:
================================================================================
Rank = 0; Score = 20971520.0
<|begin_of_text|>Bigfoot has been a staple of American folklore since the 19th century. Many people are convinced that Bigfoot is real. Others suggest that he is a cultural phenomenon. Some just want to believe. There is even a group, the Bigfoot Field Researchers Organization, that tracks Bigfoot sightings. And they have thousands of reports available online that date back to the late 19th century. The Internet, it seems, has everything. So, I took this data, all 4,586 records of it, and used it to build a classifier. It was a good model with pleasing metrics. I liked it. But then what? For some folks, the model is where the work ends. But I'm a developer and that's only half the solution. I've got a model but how do I use it? How do I put it in an application so that a user can, well, use it? I'm going to answer that question in this talk, and a bit more. I'll show you how I exposed my Bigfoot classifier to the Internet as a REST-based API written in Python. And we'll tour a couple of applications I wrote to use that API: a web-based application written in JavaScript and an iOS application written in Swift. For the model itself, I'll use DataRobot since it's quick and easy. And, I work there! When we're done, you'll know how to incorporate a model into an API of your own and how to use that API from your application. And, since all my code is on GitHub, you'll have some examples you can use for your own projects. As a bonus, you'll have 4,586 Bigfoot sightings to play with. And who doesn't want that? **************************************************************** Guy started his career as gasp a COBOL programmer. But don’t hold that against him, it just gives him perspective. He has spent much of his time programming in the most popular of the semi-colon delimited languages including C++, Java, and JavaScript. More recently he has been working with Python and machine learning. In addition to programming computers, Guy had a background in electronics and enjoys building circuits, burning himself with a soldering iron, and programming small hardware devices such as the Arduino and the Raspberry Pi. No one has actually paid him to do this sort of work… yet. Guy loves to speak and teach and will go to any conference that will give him an audience and teach anyone who wants to learn. He normally speaks about technology but has been known to
================================================================================
Rank = 1; Score = 15663104.0
<|begin_of_text|>Lady Gaga is an American singer, songwriter, and actress who has received many awards and nominations for her contributions to the music industry. She rose to prominence with the release of her debut album The Fame in 2008. The album won several awards and was nominated for six Grammy Awards, including Album of the Year. The album and its single "Poker Face" won Best Electronic/Dance Album and Best Dance Recording, respectively, at the 52nd Grammy Awards. The album also won International Album at the 2010 BRIT Awards. A follow-up EP, titled The Fame Monster, was released in 2009, and included the singles "Bad Romance" and "Telephone". The music videos of the songs won eight accolades from thirteen nominations at the 2010 MTV Video Music Awards (VMA), making Gaga the most nominated artist in VMA history for a single year and the first female artist to receive two nominations for Video of the Year in one single night.[1] In 2011, Gaga was nominated for six Grammy Awards, and won three—Best Pop Vocal Album for The Fame Monster, and Best Female Pop Vocal Performance and Best Short Form Music Video for "Bad Romance".

Born This Way (2011), Gaga's second studio album, accrued three nominations at the 54th Annual Grammy Awards, including her third consecutive nomination for Album of the Year. It won the People's Choice Awards for Album of the Year the following year, and the music video for the title track won two VMAs, including Best Female Video. Her third album, Artpop (2013), won the award for World's Best Album by a Female at the 2014 World Music Awards. In other musical ventures, Gaga released a collaborative jazz album with Tony Bennett, titled Cheek to Cheek (2014), which received the Grammy Award for Best Traditional Pop Vocal Album.

In 2015, Gaga released a song "Til It Happens to You", for the documentary film, The Hunting Ground. The song won a Satellite Award for Best Original Song, while nominated for an Academy Award, a Critics' Choice Movie Award for Best Song, and a Grammy Award for Best Song Written for Visual Media. In the same year she was named Woman of the Year by Billboard. She also became the first woman to receive the Digital Diamond Award from RIAA,[2] and the first artist to win the Songwriters Hall of Fame's Contemporary Icon Award for "attaining an iconic status in pop culture". Gaga received a Golden
================================================================================
Rank = 2; Score = 15138816.0
<|begin_of_text|>If you want to make money playing poker, you have to read your opponents without letting them read you.

Stephen Harper sure is hard to read, but in the game of high-stakes poker over the election debates, he seems to have misread the other players.

On Thursday, representatives of the NDP, Liberals, Greens and Bloc agreed to do one English and one French debate to be broadcast by CTV, CBC, Global and Radio Canada, which would reach millions of Canadian TVs.

The Conservatives say they’re too busy with other debates. So far, they have agreed to debates put together by TVA, Maclean’s, the

Globe and Mail and the Munk Debates. La Presse is likely next.

The problem for the Tories is that the consortium of big broadcasters controls TV bandwidth, and they are refusing to cede control to the other media organizations.

When British Prime Minister David Cameron stayed away from a similar debate recently, the British consortium went ahead without him. That didn’t help Cameron’s rival, Labour Leader Ed Miliband, because he got beat up by the leaders of smaller parties.

The dynamic would be different here because many voters would like to have a different prime minister, but they disagree on whether it should be Thomas Mulcair or Justin Trudeau.

If the two of them square off (with Elizabeth May) in the only English debate to be broadcast on millions of TV screens, the non-Tory voters might settle on one of them. If that happens, Harper would find it hard to keep his job.

The numbers do not look good anyway. Friday’s Ekos poll for iPolitics shows the Tories in a three-way tie with the NDP and the Liberals.

Worse still for him, he has little room to grow. About 40% of both Liberal and NDP supporters identify the other party as their second choice but only 13% of Liberal supporters and 8% of NDP supporters would consider voting Conservative. And 58% of Canadians would not consider voting Conservative under any circumstances.

“If our numbers are correct, the Conservatives are in deep trouble,” said pollster Frank Graves. “They’ve got very little room for growth, and they’re 12 points back … from where they were election night.”

A security crisis might move that number, but the “balanced” budget, anti-terror talk and tax cuts haven’t done the trick.

A big chunk of voters — Graves calls them “promiscuous progressives” — want to see the back of Harper. If the biggest debate of the campaign takes
================================================================================
Rank = 3; Score = 10289152.0
<|begin_of_text|>Lottery mathematics is used to calculate probabilities of winning or losing a lottery game. It is based heavily on combinatorics, particularly the twelvefold way and combinations without replacement.

Choosing 6 from 49 [ edit ]

In a typical 6/49 game, each player chooses six distinct numbers from a range of 1-49. If the six numbers on a ticket match the numbers drawn by the lottery, the ticket holder is a jackpot winner—regardless of the order of the numbers. The probability of this happening is 1 in 13,983,816.

The chance of winning can be demonstrated as follows: The first number drawn has a 1 in 49 chance of matching. When the draw comes to the second number, there are now only 48 balls left in the bag, because the balls are drawn without replacement. So there is now a 1 in 48 chance of predicting this number.

Thus for each of the 49 ways of choosing the first number there are 48 different ways of choosing the second. This means that the probability of correctly predicting 2 numbers drawn from 49 in the correct order is calculated as 1 in 49 × 48. On drawing the third number there are only 47 ways of choosing the number; but of course we could have arrived at this point in any of 49 × 48 ways, so the chances of correctly predicting 3 numbers drawn from 49, again in the correct order, is 1 in 49 × 48 × 47. This continues until the sixth number has been drawn, giving the final calculation, 49 × 48 × 47 × 46 × 45 × 44, which can also be written as 49! ( 49 − 6 )! {\displaystyle {49! \over (49-6)!}} or 49 factorial divided by 43 factorial. This works out to 10,068,347,520, which is much bigger than the ~14 million stated above.

However; the order of the 6 numbers is not significant. That is, if a ticket has the numbers 1, 2, 3, 4, 5, and 6, it wins as long as all the numbers 1 through 6 are drawn, no matter what order they come out in. Accordingly, given any set of 6 numbers, there are 6 × 5 × 4 × 3 × 2 × 1 = 6! or 720
================================================================================
Rank = 4; Score = 8585216.0
<|begin_of_text|>On July 11, 2016, 6 months after the first published version of Zappa, Amazon announced the preview release of Chalice, now the Python Severless Microframework for AWS. This was a fairly big shock to our community at the time, as it looked as though our Free and Open Source project was being undercut by Amazon's propietary offering. Not only that, but it felt like their interface and even their presentation of the product was a̶ ̶d̶i̶r̶e̶c̶t̶ ̶r̶i̶p̶-̶o̶f̶f̶ ̶o̶f̶ inspired by our efforts, potentially designed to ~~~trick~~~ help consumers and lock them into the AWS ecosystem.

I don't want to attribute any direct malice there, (this is probably just a case of a good idea being executed in multiple ways), but Amazon does have a fairly long history of building their own proprietary or locked-in versions of software products and services built on top of AWS infrastructure.

So, now that both products have matured even further, I think it's time that we do a comparison of the two frameworks. Obviously, as one of the authors of Zappa I'm strongly biased, so take my opinions for what they are, but it is my strong belief that although Chalice has some interesting features, Zappa is the vastly superior product for these important reasons:

Zappa does not lock you in to the AWS ecosystem.

to the AWS ecosystem. Zappa has vastly more useful features.

. Zappa is battle-tested, used in production by medical, scientific and banking users.

Against Vendor Lock-In

This is the big headline here: Chalice locks you into using AWS services - Zappa doesn't.

With Zappa, you can run existing Python web applications on AWS Lambda thanks to the magic of WSGI. More importantly, you can leave AWS at any time, meaning you can run the same application on any web server your like, be it VPS, on Heroku, or on your own bare metal. (You will lose some of the event-driven benefits which AWS can provide, but you will always have the option to use other replacements if that's what you decide.)

With Chalice, you will have to re-write your application to use the Chalice framework, and after that it will only ever be able to be run on AWS infrastructure.


================================================================================
Rank = 5; Score = 8519680.0
<|begin_of_text|>Renting Dumpsters Posted on December 5, 2018 | By STEVE Whenever people have a task, it is easy to generate waste and piling up garbage can be a main source of worry. This is not only with regards to taking up space but also in conditions of polluting the environment unnecessarily. Therefore, how perform these worries are taken by you aside? This can be done by opting to use 20 yard dumpsters philadelphia simply. These are offered by local rental businesses and are designed to alleviate the worries of coping with trash from your brain. There are many of these rental companies in the marketplace and as such, it ought not to be difficult so that you can get the same. The local rental businesses will make sure that you recycle your waste in an eco-friendly method and at the same period, make the process easy. By using these eco-friendly dumpsters, you not really just ensure that your encircling is clean and gives you the ideal life-style but it will also make sure that you enjoy great wellness. As a result, this will give you great endurance and boost your probabilities of taking pleasure in a treatment free of charge way of living. These can be utilized in different fronts and places such as churches, house owners, areas and additional businesses. For this good reason, they are known to provide all round services to all these combined groups. There are instances when you might have dangerous waste products and this provides the ideal chance that you can ensure that you get rid of the same in a secure way. Dumpsters may end up being availed in various designs seeing that good as sizes and this makes it all easy for individuals to help to make buys based on their requirements. Another main benefit connected with this type of waste materials disposal is usually the truth that the consumer can place it in an area of their choice. This makes it simple to make sure that the clean up process is simple and effective on your part as well. It is normally essential to take note that the rental companies are also accountable for making sure that they are eliminated from your property once you are completed using the same and they can adhere to your time routine. What is more, in many situations, these ongoing companies will adhere to certain safety procedures when they are putting the same on site. At this true point, it is important to note that dumpsters that are provided by professional local rental businesses are also the perfect way to eliminate waste that is produced on building sites whether it is green backyard, commercial or home keep waste. What is usually even more, this provides you
================================================================================
Rank = 6; Score = 8519680.0
<|begin_of_text|>Today, we are announcing the general availability of App Service Isolated, which brings the simplicity of multi-tenant App Service to the secure, dedicated virtual networks powered by App Service Environment (ASE).

Azure App Service is Microsoft’s leading PaaS (Platform as a Service) offering hosting over 1 million external apps and sites. It helps you build, deploy and scale web, mobile and API apps instantaneously without worrying about the underlying infrastructure. It allows you to leverage your existing skills by supporting an increasing array of languages, frameworks, and popular OSS, and has built-in capabilities that streamline your CI/CD pipeline. ASE was introduced in 2015 to offer customers network isolation, enhanced control, and increased scale options. The updated ASE capabilities that comes with the new pricing tier, App Service Isolated, now allow you to run apps in your dedicated virtual networks with even better scale and performance through an intuitive user experience.

Streamlined scaling

Scaling up or out is now easier. The new ASE eliminates the need to manage and scale worker pools. To scale, either choose a larger Isolated plan (to scale up) or add instances (to scale out), just like the multi-tenant App Service. It’s that easy. To further increase scaling flexibility, App Service Isolated comes with a maximum default scale of 100 Isolated plan instances. You now have more capacity for large implementations.

Upgraded performance

The new ASE uses dedicated Dv2-based machines boasting faster chipsets, SSD storage, and twice the memory per core when compared to the first generation. The dedicated worker sizes for the new ASE are 1 core with 3.5 GB RAM, 2 cores with 7 GB RAM, and 4 cores with 14 GB RAM. With this upgraded infrastructure, you will be able to run your apps with lower latency, have more power to handle heavier workloads, and support more users.

Simplified experience

Creating the new ASE is easy. By selecting the Isolated pricing tier, App Service will create an App Service Plan (ASP) and a new ASE directly. You will just need to specify the Virtual Network that you want to deploy your applications to. There is no separate workflow required to spin up a new ASE in your secure, dedicated virtual networks.

We’ve made App Service Environment (ASE) faster, more efficient, and easier to deploy into your virtual network, enabling you to run apps with Azure App Service at high scale in an isolated network environment. Check out the Azure Friday video. Partners and customer can also
================================================================================
Rank = 7; Score = 8323072.0
<|begin_of_text|>Only recently the Cockpit project was launched, aiming at providing a web based management interface for various servers. It already leaves an interesting impression for simple management tasks – and the design is actually well done.

I just recently came across the only three month old Cockpit project. The mission statement is clear:

Cockpit is a server manager that makes it easy to administer your GNU/Linux servers via a web browser.

The web page also states three aims: beginners friendly interface, multi server management – and that there should be no interference in mixed usage of web interface and shell. Especially the last point caught my attention: many other web based solutions introduce their own magic, thus making it sometimes tricky to co-administrate the system manually via the shell. The listed objectives also make clear that cockpit does not try to replace tools that go much deeper into the configuration of servers, like Webmin, which for example offers modules to configure Apache servers in a quite detailed manner. Cockpit tries to simply administrate the server, not the applications. I must admit that I would always do such a application configuration manually anyway…

The installation of Cockpit is a bit bumpy: besides the requirement of tools like systemd which limits the usage to only very recent distributions (excluding Ubuntu, I guess) there are no packages yet, some manual steps are required. A post at unshut.me highlights the necessary steps for Fedora which I followed: in includes installing dependencies, setting firewall rules, etc. – and in the end it just works. But please note, in case you wanna give it a try: it is not ready for production. Not at all. Use virtual machines!

What I did see after the installation was actually rather appealing: a clean, yet modern web interface offering the most important and simple tasks a sysadmin might need in a daily routine: quickly showing the current health state, providing logs, starting and stopping services, creating new users, switching between servers, etc. And: there is even a working rescue console!

Main login view Default server overview, showing the possible tasks as well as the current health of the system. Default server view in wide screen browser Detail view of host name and system basics Network traffic – a click onto on of the health statistics brings up a more detailed view. Listing services – here it becomes obvious how much Cockpit actually is build upon systemd. The log watcher accesses the systemd journal – once again a reason why systemd is a requirement for Cockpit. Adding a new server to Cockpit. Two servers are registered and can be accessed in
================================================================================
Rank = 8; Score = 7897088.0
<|begin_of_text|>Share The Latest News

Earlier this year LG, in partnership with Valve, showcased a prototype of their upcoming LG VR headset at GDC 2017 which is now rumored to be called UltraGear. So far we know that the upcoming headset will boast some new specs that differ from current headset manufacturers.

LG UltraGear VR Headset Specifications:

Two panels (one for each eye) with a resolution of 1440 x 1280 each

OLED display from LG

3.64 inches diagonal

90 Hz refresh rate

110 degree FOV

With new reports from Letsgo Digital, we have discovered that LG has filed new patents that might greatly change how users will put on and take off the upcoming LG VR headset. Through the images seen in the patent, it clearly outlines how the headset can be opened in the middle which allows users to rest it on their shoulders if they so choose.

This makes it easier to take quick rests from VR and also makes it easier to maneuver around if you had to take off the headset for any reason. We have not yet seen any major headset manufacturer do this where the HMD can be split down the middle.

In addition, there will be a knob on the back which will allow you to control the sizing of the headset for your head which was a great feature added on the PSVR and the new HTC Vive deluxe head straps.

There will also be an area where it looks like the headset will allow you dock your earbuds on the back. We aren’t quite sure how this works but it looks as though that it pulls in and out.

You can find the rest of the images here for the recently filed patents. This does not mean that this will be final but it’s quite an interesting design. It might also be possible that LG will upgrade the currently known specs such as the display panels and FOV.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 9; Score = 7602176.0
<|begin_of_text|>Will White of Mapbox shared the following guest post with me. In the post, Will describes how they use EC2 Spot Instances to economically process the billions of data points that they collect each day.

I do have one note to add to Will’s excellent post. We know that many AWS customers would like to create Spot Fleets that automatically scale up and down in response to changes in demand. This is on our near-term roadmap and I’ll have more to say about it before too long.

— Jeff;

The largest automotive tech conference, TU-Automotive, kicked off in Detroit this morning with almost every conversation focused on strategies for processing the firehose of data coming off connected cars. The volume of data is staggering – last week alone we collected and processed over 100 million miles of sensor data into our maps.

Collecting Street Data

Rather than driving a fleet of cars down every street to make a map, we turn phones, cars, and other devices into a network of real-time sensors. EC2 Spot Instances process the billions of points we collect each day and let us see every street, analyze the speed of traffic, and connect the entire road network. This anonymized and aggregated data protects user privacy while allowing us to quickly detect road changes. The result is Mapbox Drive, the map built specifically for semi-autonomous driving, ride sharing, and connected cars.

Bidding for Spot Capacity

We use the Spot market to bid on spare EC2 instances, letting us scale our data collection and processing at 1/10th the cost. When you launch an EC2 Spot instance you set a bid price for how much you are willing to pay for the instance. The market price (the price you actually pay) constantly changes based on supply and demand in the market. If the market price ever exceeds your bid price, your EC2 Spot instance is terminated. Since spot instances can spontaneously terminate, they have become a popular cost-saving tool for non-critical environments like staging, QA, and R&D – services that don’t require high availability. However, if you can architect your application to handle this kind of sudden termination, it becomes possible to run extremely resource-intensive services on spot and save a massive amount of money while maintaining high availability.

The infrastructure that processes the 100 million miles of sensor data we collect each week is critical and must always be online, but it uses EC2 Spot Instances. We do it by running two Auto Scaling groups, a Spot group and an On-Demand group, that share a single Elastic Load
================================================================================
Rank = 10; Score = 7405568.0
<|begin_of_text|>In the last year, Progressive Web Apps have become an incredibly popular way to build next generation apps. PWAs bring many advantages to the table, and in today’s post I’d like to share why we think you should care.

Why should I care about PWAs?

PWAs bring unique advantages to the table for developers building consumer facing apps. They allow developers to completely skip the app stores and instead simply deploy to a web server. This allows you to get your app out faster and to more people than you would be able to with deploying to the App Store and Google Play. Also, because you are not tied to the app store, this means that updates can be immediately available to everyone using your app, by simply pushing your new code to your web server. And because PWAs run in the browser, your single PWA can then be reached from just a click on a URL, greatly reducing the barrier to entry to your app!

PWAs also bring a ton of new advantages to the table for businesses and enterprises. Because of the fact that with a PWA you don’t have to package your app for any specific native platform, or submit that app to any native app store, your time to market can be much faster. Just push to a web server and your PWA is available to everyone.

For users of your app, PWAs offer a consistent experience no matter what device they are using, or how slow or flaky their network connection is. There’s no need to sit and wait while your app they are wanting to use is downloading and installing. Need something in a hurry? There is nothing much faster than tapping a URL. Also, because PWAs tend to be much smaller than the average native app, your app will not be taking up much of the precious storage on a user’s device.

How to get started today

The first concern I normally hear from devs interested in building a PWA is that Safari, the preinstalled browser on Apple devices, does not have support for PWAs. To be frank, this actually doesn’t matter because of progressive enhancement. With a few extra lines of code you can assure that users on browsers that do support those API’s get the full experience, and users on safari still get a great,fast experience. Also, Webkit, the browser engine behind Safari, recently announced they were implementing service workers, a key api for PWAs, so these API’s are going to be coming to Safari soon anyway.

Ionic fully supports PWAs out of the box. Our starters provide the minimum things
================================================================================
Rank = 11; Score = 7208960.0
<|begin_of_text|>Data Warehousing at Wayfair

In 2009 Wayfair’s database infrastructure was based almost entirely on Microsoft SQL Server. Our Business Intelligence team was using a SQL Server data warehouse to prepare a large amount of data for import into Analysis Services (SSAS) each day. We populated our data warehouse using transaction log shipping from production servers, which required about 3 hours of downtime on the data warehouse at midnight each night to restore the previous day’s logs. Once that was done, a series of stored procedures were kicked off by jobs that would crunch through data from several different servers to produce a star schema that could be pulled into SSAS. Wayfair was scaling rapidly, and this approach started to become painfully slow, often taking 10-16 hours to crunch through the previous day’s data.

The BI team decided to look into other solutions for data warehousing, and ultimately purchased a Netezza appliance. Netezza is essentially a fork of PostgreSQL that takes a massively parallel cluster of nodes (24 in our case) and makes them look like one database server to the client. In our tests, Netezza could crunch through our data in roughly a quarter of the time, bringing 10-16 hours down to a much more reasonable 2-4 hours. The dream of updating our data warehouse multiple times each day was starting to look feasible. The feedback loop on business decisions would become dramatically shorter, enabling us to iterate more quickly and make well informed decisions at a much faster pace. There was just one glaring problem.

Great, But How Are We Going to Get Data Into It?

As soon as the DBA team heard that the Netezza purchase had been finalized, our first question was “great, but how are we going to get data into it?” The folks at Netezza didn’t have an answer for us, but they did send us an engineer to help devise a solution. As it turned out, the problem of how to incrementally replicate large amounts of data into a data warehouse was a common one, and there were surprisingly few open source solutions. Google it, and most people will tell you that they just reload all their data every day, or that they only have inserts so they can just load the new rows each day. “Great, but what if you want incremental replication throughout the day? What if you have updates or deletes? How do you deal with schema changes?” Crickets.

The First Solution

The solution we arrived upon was to use SQL Server Change Tracking to keep track of which rows had
================================================================================
Rank = 12; Score = 6586368.0
<|begin_of_text|>More Info

Windows 10

Microsoft Windows 10 combines the best elements of the Windows you already know, like the Start menu, with new features like space to pin your favorite apps and easy navigation so you'll feel right at home. Getting set up is faster and easier so you can get right to work or exploring the Windows App Store games or a new version of Office. InstantGo quickly boots up or resumes Windows while enhancements help balance memory and processor resources for maximum efficiency and Battery Saver automatically conserves power. Windows 10 also offers more built-in security features improving protection against viruses, phishing, and malware.

Solid State Drive

More reliable and significantly faster than traditional spinning-platter hardrives, solid state drives work more like a large flash drive giving you quick access to your data. With no moving parts generating heat, solid state drives use less power and keep your system cooler which helps reduce component failure. Light weight and durable, solid state drives are often found in portable devices since they are less prone to travel damage and accidents like being dropped.

1TB Hard Drive

A terabyte equals one trillion bytes. To put that in perspective, that's room enough for about 250,000 MP3 files, or according to Ron White, in his book, "How Computers Work", 20,000 four-drawer filing cabinets filled with text. In addition, this hard drive operates at a variable RPM which means you've got the best of both worlds, full speed for demanding loads and energy conservation during minimal access.

Smart Cache

With Intel Smart Cache you benefit from increased data access because the cache is shared between the cores from a single access point and optimized by workload demand. That means your system is making maximum use of its resources, enhancing multi-threading, and reducing storage redundancy.

Intel® Quick Sync Video

Built into every 2nd generation Intel Core™ processor, Quick Sync Video enables users to quickly create, edit, synchronize, and share video files from home or make them available online without the need for extra hardware. In addition, media conversion between devices takes just minutes thanks to hardware acceleration streamlining this process at incredible speed. That includes creating DVDs, Blu-ray™ discs, or editing and converting HD videos for portable media devices or uploading to your favorite social media sites.

Intel InTrue™ 3D Technology

The perfect way to watch 3D movies. InTrue 3D technology delivers 1080p, hi-def resolution 3D action to your TV using HDMI 1.4. With Blu
================================================================================
Rank = 13; Score = 6520832.0
<|begin_of_text|>In the last few years, sites like Kickstarter and Indiegogo have revolutionized the world for startups. Ideas that didn't have mainstream business appeal were not only getting off the ground, but flourishing. Blockchain crowdfunding might just be the next step in startup evolution, helping important and interesting projects come to fruition.

Crowdfunding provides an excellent way for creative projects to find cash. Many small or obscure projects lay outside of the scope of traditional investment. This makes it very difficult for them to get their ideas off the ground. Kickstarter, Indiegogo, and others changed that, by allowing startups to connect directly with potential consumers in order to seek funding. They acted as a trusted third party to keep the money in escrow, helping to protect potential investors from being scammed. If the funding succeeds, customers get whatever was promised to them as part of the campaign. If not, their money will be sent back.

Blockchain Meets Kickstarter: Is This The Future?

The problem with these established crowdfunding companies is that they are centralized bodies, charging high fees and also influencing the projects. Blockchain-based crowdfunding is set to be a game changer because it decentralizes the funding model from the likes of Kickstarter and other companies.

Kickstarter provides a service and there are costs to run that service, so it's hard to blame them for charging 5% of the total funds received, with an additional 3-5% going towards payment processing. Despite this, it is still expensive. With blockchain's distributed ledger, there is the potential to remove this third party, which will save a considerable amount of the fundraising costs.

Blockchain crowdfunding works by allowing startups to create their own digital currencies and sell them. This allows them to raise funds from early investors, while the investors also have the potential to make money if the value of their cryptographic shares increases.

Some advocates consider this a more pure form of crowdfunding, because it removes any intermediaries standing between the backers and the startup. It also has the potential to boost new blockchain platforms, because it will give the blockchain community a new way to fund its own projects. It appeals to the more anarchistic blockchain enthusiasts as well, because it allows them to avoid traditional funding methods.

How Does Blockchain Crowdfunding Work?

It is similar to Kickstarter, with the creators posting their project and then soliciting funds from a community of interested people. Where it differs is that the startups will be able to make their own cryptocurrency to sell to potential backers. These cryptocurrency tokens will be accounted for and kept track of by the blockchain, which makes it immutable
================================================================================
Rank = 14; Score = 6356992.0
<|begin_of_text|>About

Sayid Pro Brings Transparency to the Production Environment

Sayid Pro is a tool for debugging and profiling Clojure in a production environment. We all want more visibility into what is happening in our production servers. When something is going wrong, we want precise answers. Sayid Pro aims to give you that.

Sayid Pro works by tracing functions. When a traced function is executed, it's arguments and return value are captured by Sayid Pro. Sayid Pro provides a web interface through which you can remotely activate tracing on any functions, on any of your servers (that have Sayid Pro embedded in them). The Sayid Pro web application also allows users to query and visualize the captured trace data.

You can also watch a hi-res version of the video here: https://youtu.be/y5ll-6IjJGw

Architecture

Library - The Sayid Pro Library is included in your production servers. It handles data capture and transmission.

Hub - The Sayid Pro Hub runs in your infrastructure. It communicates with servers running the Sayid Pro library and receives trace data from them. Trace data is written to a PostgreSQL database as it is received. The hub also hosts the web app.

Database - All captured trace data is stored in a PostgreSQL database.

Web App - The web app is hosted by the Sayid Pro Hub. This is the interface that allows you to toggle tracing and then query and visualize captured trace data.

Web App UI

The web application is a critical piece of the product. I hope the prototype, while still immature, demonstrates the potential of Sayid Pro as a tool for solving production problems. Below is a description of the major UI components as they currently exist.

Query Editor

This is where it all starts. The query editor is for composing and executing queries and commands. These are sent off to the Sayid Pro Hub for execution. Some commands, such as enabling or disabling tracing, are forwarded on to servers. Servers can be selected individually, or in groups by a class tag.

Trace data can be queried by different fields, such as end time or function name. Trace data is rendered into interactive visualization in the tree, table and chart UI components. Those are described below.

Tree

The tree UI component represents the trace data in a way that reflects its original call hierarchy. Each instance of a function call is a node. These nodes show the name of the function. The tree will also display the arguments and return value of a trace instance when the node is clicked.

Table

The table
================================================================================
Rank = 15; Score = 6324224.0
<|begin_of_text|>For the RAC release, here’s some of the metadata that was automatically generated using a system we developed called Constellate (that will become the foundational layer of our upcoming creator’s portal):

Along with content-addressing, this layer then becomes the starting point for licensing a work.

COALA IP forms part of the CBOR IPLD specification that will allow us to in the future traverse this graph from metadata object right into Ethereum and other merkle data structures. The website includes access to the open source implementations.

With that, a group was formed in October 2015 to work such a standard. Ujo, BigchainDB, IPFS, Swarm, Jaak and many others collaborated over the course of the past year and half to establish such a standard. We didn’t reinvent the wheel and adapted existing standards to be more blockchain friendly, including hash linking that we’ve come to know and love: creating COALA IP.

One of the key issues for a machine-readable licensing system: is to have a standard that’s easily readable and digestible. This makes it easier for developers to license works for their applications.The Tiny Human Ethereum Smart Contract had rights information in it, but it was embedded in an Ethereum smart contract. Besides having to run a full Ethereum node to get access to the data, there was also no standard API to interface with it back then. We also have to consider a substantially more long-term future, where it could be possible that specific blockchain implementations can morph and change over time. A key design consideration was splitting of the data (rights owners, from the business logic: paying, rights transfers, etc). The data can then be referenced by any current or future blockchain.

On May 22nd, RAC announced that he wanted to release his album on the Ethereum network. We saw it as a good chance to demonstrate the potential of our architecture with him. In this collaboration, we are about 75% of the way towards finalising the different parts of this infrastructure. In this initial iteration, we had to manually craft the various components of the system, but also structured it in a modular and reusable way. In the future, with the introduction of our creator’s portal, we aim to automate this, and provide access to all artists.

We’ve devised a system that improves the way current licensing is handled, but also opens up a large new scope for innovative and flexible licensing schemes. We saw glimpses of this when we launched the Tiny Human prototype with Imogen Heap in October 2015: a smart contract
================================================================================
Rank = 16; Score = 6193152.0
<|begin_of_text|>It’s not difficult to read and listen about the wonders of Embarcadero DataSnap technology around the world. I attended the Delphi Conference 2011 and 2012 in Sao Paulo, Brazil, after the release of versions of Delphi XE2 and XE3, and the words “performance” and “stability” are not spared when the subject is DataSnap. Apparently a perfect solution for companies that have invested their lives in Delphi and now need to reinvent themselves and offer web and mobile services. But does DataSnap really work on critical conditions?

I found some references from other people talking about it on StackOverflow:

The company I work for stands as one of the five largest in Brazil when it comes to software for supermarkets. The main product is a considerably large ERP developed in Delphi (for over ten years). Recently we initiated a study to evaluate technologies that would allow us to migrate from client / server to n-tier application model. The main need was to be able to use the same business logic implemented on a centralized location on different applications.

The most obvious option is the DataSnap, which works very similarly to what is found in legacy applications. It can drag components, using dbExpress, for example.

We tested the DataSnap technology in order to know what level of performance and stability it provides and to verify if it really meets our requirements. Our main requirement was the server’s ability to manage many simultaneous connections, since the application is big and used by many users.

Objective

Our objective was to test the DataSnap REST API and to answer some questions:

How does it behave in an environment with many concurrent connections?

What is the performance in a critical condition?

Is it stable in critical condition?

Methodology

The tests are based on a lightweight REST method without any processing or memory allocation, returning only the text “Hello World”. The DataSnap server was created using the Delphi XE3 wizard and implemented the HelloWorld method. The testing was performed on all types of lifecycles (Invokation, Server and Session) and also with VCL and Console application. However, as the results were identical, the presented data is only based on the results obtained using the console version with “Server” lifecycle only.

We decided to create a few servers based on other technologies, not necessarily similar or with the same purpose, just to have a basis for comparison. Servers have been created using the following frameworks:

mORMot (Delphi)

ASP.NET WCF

Jersey/Grizzly (Java
================================================================================
Rank = 17; Score = 6127616.0
<|begin_of_text|>Take virtually any modern day SSD and measure how long it takes to launch a single application. You’ll usually notice a big advantage over a hard drive, but you’ll rarely find a difference between two different SSDs. Present day desktop usage models aren’t able to stress the performance high end SSDs are able to deliver. What differentiates one drive from another is really performance in heavy multitasking scenarios or short bursts of heavy IO. Eventually this will change as the SSD install base increases and developers can use the additional IO performance to enable new applications.

In the enterprise market however, the workload is already there. The faster the SSD, the more users you can throw at a single server or SAN. There are effectively no limits to the IO performance needed in the high end workstation and server markets.

These markets are used to throwing tens if not hundreds of physical disks at a problem. Even our upcoming server upgrade uses no less than fifty two SSDs across our entire network, and we’re small beans in the grand scheme of things.

The appetite for performance is so great that many enterprise customers are finding the limits of SATA unacceptable. While we’re transitioning to 6Gbps SATA/SAS, for many enterprise workloads that’s not enough. Answering the call many manufacturers have designed PCIe based SSDs that do away with SATA as a final drive interface. The designs can be as simple as a bunch of SATA based devices paired with a PCIe RAID controller on a single card, to native PCIe controllers.

The OCZ RevoDrive, two SF-1200 controllers in RAID on a PCIe card

OCZ has been toying in this market for a while. The zDrive took four Indilinx controllers and put them behind a RAID controller on a PCIe card. The more recent RevoDrive took two SandForce controllers and did the same. The RevoDrive 2 doubles the controller count to four.

Earlier this year OCZ announced its intention to bring a new high speed SSD interface to the market. Frustrated with the slow progress of SATA interface speeds, OCZ wanted to introduce an interface that would allow greater performance scaling today. Dubbed the High Speed Data Link (HSDL), OCZ’s new interface delivers 2 - 4GB/s (that’s right, gigabytes) of aggregate bandwidth to a single SSD. It’s an absolutely absurd amount of bandwidth, definitely more than a single controller can feed today - which is why the first SSD to support it will be a multi-controller device with internal RAID.

Instead of relying on
================================================================================
Rank = 18; Score = 6029312.0
<|begin_of_text|>Working in a global retail environment poses some interesting availability challenges when you have physical Bricks and Mortar stores. I have been thinking about the problem of high availability in this environment for a little while now due to a project I am involved with to harmonise the retail systems used between global groups. It is quite a common problem for an organisation that has grown through acquisition that you have different systems used in different business units, but after a while it makes sense to try and go with a common platform.

This article talks about how this architecture could look and how you can support the staggered roll-out of new Point of Sale features to the store whilst still maintaining high availability.

Logical View

Before I talk about the architecture, I want to cover a scenario first of the end state. Imagine there is a global retail company based in both North America and Europe. Both territories have around 1000 physical bricks and mortar stores. These stores each have a number of tills (cash registers for my American friends). There could be between 2 and 5 tills per store depending on its size. Each till communicates with systems hosted at a centralised location. These systems consist of web services, caching servers and databases. This has been illustrated in the diagram below.

This diagram shows 2 geographic regions. Each Region contains a head office network infrastructure and a store network infrastructure. Both of the global regions are completely separate from each other. There are no shared resources between the two.

A requirement for this architecture is that it be highly available. The stores should always have access to the services. The easiest thing to do is to put a load balancer in front of the web services and have multiple app servers. Then if any app server should go down or exhibit problems, the other load balancers can pick up the slack.

Physical Architecture

Although the App Servers described are load balanced and provide a degree of availability, if there is a problem/outage with the data centre where they are hosted, then you have a problem as no matter how many load balanced App Servers you have, if connectivity goes to the data centre then your retail stores are cut off from working. The way around this is to have co-location facilities in each region. This is shown in the diagram below.

Each global region has 2 locations. There are 2 ways you can run this configuration.

Active – Active: This is where you run both locations at the same time and load balance between the 2 of them. If one location goes down the other location picks up the traffic.


================================================================================
Rank = 19; Score = 5963776.0
<|begin_of_text|>Deploying a Ruby app with Passenger to production

Passenger is an open source web application server for Ruby. It handles HTTP requests, manages processes and resources, and enables administration, monitoring and problem diagnosis. Passenger is very easy to use, makes deploying in production much easier and is scalable.

What is this tutorial?

This is an end-to-end tutorial that teaches you how to install Ruby and Passenger on a production server. This tutorial will ask you some questions, so the exact tutorial steps depend on the choices you make. In general, it covers these topics:

Launching a server Installing the Ruby runtime Installing Passenger Setting up the production server in preparation for the application, such as creating a Unix user account and setting up permissions Deploying the application itself Updating the application to newer versions Optional integration with a web server, such as Nginx or Apache

Not familar with Passenger? Please take a look at the quickstart and the basics tutorial. Developing? Looking to run Passenger in development? Then please read the basics tutorial or the development guide. Advanced user? Simply skip the steps that you are already familiar with, or refer to our installation guides which are not end-to-end, but are shorter and more focused.

Ready to get started?

Please start by picking the hosting provider or infrastructure that you want to deploy to.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 20; Score = 5963776.0
<|begin_of_text|>Yesterday I read this article, from Derek Sivers’ blog. I encourage you to read it as well, but here’s the summary:

We should move logic more into the database, because “Classes, models, and methods (OOP) are an unnecessary complication.”

Data structure doesn’t change often, but code (logic) does. So let’s move the logic closer to the data structure

“PL/pgSQL is not the most beautiful language, but the simplicity of having everything in the database is worth it.”

We can do that by using triggers, stored procedures and the likes. We then call them directly from the application layer (there follows an example of a REST api in sinatra).

First I thought it was a long winded joke, but the article was over before the expected “haha jk”. I actually had to scroll down quite a bit to find the first dubious commenter.

Now, let’s clarify one thing: his approach makes sense – under very specific conditions. Those conditions loosely being that the application is mostly data centric (data in, data out, CRUD and a little validation), and most of the work is related to how data is presented.

Fact is, we have already the perfect solution for this scenario, and we have had it for years: it’s called Microsoft Access.

So, why aren’t all applications out there written in MS Access, or one of its clones?

Enter: design space

So, what is the problem with having all the logic in the database?

Anyone who’s actually worked on a big application, with hundreds of triggers and stored procedures, knows perfectly what the problem is. It might be slightly more difficult to articulate it in clear words – I myself could not place a finger on the definition for a long time.

When you decide to store all logic in the database, you have made the decision to fight in a very tight space. Everytime something unexpected pops up, you have to work it out inside the very cramped code room you’ve created for yourself.

The advantage is that everything is in reach – add, delete, change a trigger here and a stored procedure there, you can alter a great deal of things with very little code.

The disadvantage is that you’re effectively coding yourself into a corner. Moving is impossible except by making very complicated contorsions. Everytime you do, you’re at risk of breaking something you didn’t want to touch at all. Side effects galore.

And, the more you do this, the more space you’re taking away from yourself –
================================================================================
Rank = 21; Score = 5636096.0
<|begin_of_text|>Pixode, a company that started in 2010 making mobile games, just announced Tuesday the launch of what it describes as a “Crypto 2.0” platform in Open Assets and a colored coins wallet called Coinprism

“Colored coins are about taking a fraction of a Bitcoin, and tagging it with a secondary, user-defined value,” founder Flavien Charlon said in a release. “That value can then be stored on the block chain, and transferred simply by transferring the underlying bitcoins.

“This opens a wide range of possibilities. An interesting use case is the ability for a company to issue shares in the form of colored coins, which can then be traded in a purely decentralized way on the Bitcoin block chain. This allows companies to run IPOs on the block chain in minutes, and for the price of a Bitcoin transaction.”

The Open Assets platform could also be applied to smart contracts or precious metals, he said.

Coinprism is one application of Open Assets, a wallet for storing, sending and receiving colored coins. wallet. “Coinprism has a strong focus on security, and private keys are always encrypted on the client before being transmitted to the server,” Charlon said.

Pixode last year also released Predictious, a predictions and derivatives market.

We reached out to Charlon recently to get some details about the projects.

Cointelegraph: What are some other applications you imagine for Coinprism?

Flavien Charlon: Bitcoin exchanges could decide to allow people to withdraw and deposit so-called “USD coins.” Those "USD coins" would be each worth one dollar, redeemable at that exchange. People can then start using that as a payment worth exactly one dollar, so they are not exposed to the volatility of Bitcoin while still getting all the benefits of a decentralized network. This can also allow people to do arbitrage between exchanges without even having to go through the traditional banking system.

As a second example, colored coins can be used as a way to do decentralized and truly anonymous voting. A company like Coca-Cola could distribute "vote coins" by putting a private key on each bottle of Coke they sell, each with one "vote coin." Then at some point, when trying to decide what new product to put to market, they can ask their customers to send their vote coins to either one address if they want the new flavor to be X or to a different address if they prefer the new flavor to be Y. They can then simply count the number of "vote coins" on each address to know
================================================================================
Rank = 22; Score = 5537792.0
<|begin_of_text|>Last week, I wrote A Beginner’s Guide To MVC For The Web. In it, I described some of the problems with both the MVC pattern and the conceptual “MVC” that frameworks use. But what I didn’t do is describe better ways. I didn’t describe any of the alternatives. So let’s do that. Let’s talk about some of the alternatives to MVC…

Problems With MVC

Let’s restate the fundamental problems we talked about that exist with MVC:

MVC Is Stateful It only makes sense if the View, as well as the View-Model binding is stateful (so the Model can update the View when it changes) MVC Has No Single Interpretation Every framework uses their own nuanced version. How Does Logging Fit In? Where does application code that’s not clearly data-centric belong in the application? Siblings To MVC

There are a whole bunch of siblings to MVC that take slight divergences and have narrow differences. Let’s briefly talk about a few of them:

This is quite similar to the MVC pattern, except that you can nest the triads together. So you can have one MVC structure for a page, one for navigation and a separate one for the content on the page. So the “top level” dispatches requests down to navigation and content MVC triads.

This makes structuring complex pages easier, since it allows you to create reusable widgets. But it brings all of the problems that MVC has, and solves none of them (it just adds complexity on top).

So HMVC doesn’t really solve our problems…

The difference between MVC and MVVM is a lot more subtle. The basic premise is that in normal MVC, it’s bad that the View is doing two jobs: presentation and presentation data logic. Meaning that there’s a difference between actual rendering, and dealing with the data that will be rendered. So MVVM splits the MVC View in half. The presentation (rendering) happens in the View. But the data component lives in the ViewModel.

The ViewModel can interact with the rest of the program, and the View is bound to the ViewModel. This means that there’s more of a separation between presentation and the application code that lives in the Model.

The controller isn’t mentioned, but it’s still in there somewhere.

Again, this solves some types of problems with MVC, but doesn’t address any of our issues.

MVP is a bit different from MVC in implementation. Instead of having the Controller intercept user interaction and the View render data, MVP structures itself a bit differently
================================================================================
Rank = 23; Score = 5373952.0
<|begin_of_text|>[Haskell-cafe] ANNOUNCE: wai-0.0.0 (Web Application Interface)

Hello all, I'd like to announce the first release of the Web Application Interface package[1]. The WAI is a common protocol between web server backends and web applications, allowing application writers to target a single interface and have their code run on multiple server types. There are two previous implementations of the same idea: the Network.Wai module as implemented in the Hyena package, and Hack. Some distinguishing characteristics of the wai package include: * Unlike Hack, this does not rely upon lazy I/O for passing request and response bodies within constrained memory. * Unlike Hyena, the request body is passed via a "Source" instead of Enumerator, so that it can easily be converted to a lazy bytestring for those who wish. * Unlike both, it attempts to achieve more type safety by have explicit types of request headers, response headers, HTTP version and status code. * It also removes any variables which are not universal to all web server backends. For example, scriptName has been removed, since it has no meaning for standalone servers. This package also contains separate modules for conversions to and from Sources and Enumerators. I am also simultaneously releasing two other packages: web-encodings 0.2.4[2] includes a new function, parseRequestBody, which directly parses a request body in a WAI request. It handles both URL-encoded and multipart data, and can store file contents in a file instead of memory. This should allow dealing with large file submits in a memory-efficient manner. You can also receive the file contents as a lazy bytestring. Finally, wai-extra 0.0.0 [3] is a collection of middleware and backends I use regularly in web application development. On the middleware side, there are five modules, including GZIP encoding and storing session data in an encrypted cookie (ClientSession). On the backend side, it includes CGI and SimpleServer. The latter is especially useful for testing web applications, though some people have reported using it in a production environment. All of the code is directly ported from previous packages I had written against Hack, so they are fairly well tested. As far as stability, I don't expect the interface to change too drastically in the future. I am happy to hear any suggestions people have for moving forward, but expect future versions to be mostly backwards compatible with this release. Also, future versions will respect the Package Versioning Policy
================================================================================
Rank = 24; Score = 5341184.0
<|begin_of_text|>Revolut, a financial technology start-up, has started to use IBAN format for account numbers that can be used by financial institutions to credit funds into the beneficiary’s account in 42 European countries, announced company.

Lithuanian IBAN account numbers will be assigned to Revolut customers so that they will have a euro account number in any country in Europe.

All payments executed by traditional banks, including salaries, can now be made into Revolut customers’ accounts with IBAN format. In addition, this functionality will allow customers to withdraw cash from PayPal, Lydia or P2P accounts. A euro account can be opened in less than one minute on a smartphone using the Revolut application.

This financial technology start-up, known in some quarters as the “hooligan of banking”, has already attracted investment worth USD 66 million and has more than 700,000 users.

The central bank of Lithuania, Lietuvos bankas, has made an unprecedented promise to FinTech companies by announcing it will provide preliminary answers to financial institution licence enquiries within one week, the fastest turnaround in the EU.

The programme will apply to companies that are already licensed in another EU country and want to move their place of residence to Lithuania. Full authorisation will be issued in two to six months later (depending on the type of licence).

The Bank of Lithuania also recently launched the programme Newcomer, which aims to facilitate FinTech companies setting up in Lithuania by providing consultations and guiding companies throughout the licensing procedures. Companies can apply with their requests to Newcomer@lb.lt and expect an answer within two to three working days.

A range of other advantages are on offer to FinTech companies choosing to set up in Lithuania. Payment and electronic money institutions in Lithuania can access the Single Euro Payment Area (SEPA) through the infrastructure of the Bank of Lithuania, enabling them to reduce their dependence on banking competitors and offer a wider range of payment services.

Moreover, the Bank of Lithuania allocates a code to providers of payment services that is used to generate accounts in IBAN format no later than the following business day.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 25; Score = 5308416.0
<|begin_of_text|>The fundamental goal of typography is to make text easy and enjoyable to read. Typography has its own set of rules and guidelines. From there, we bend the rules to prioritize how to make the text easier to read.

Treating text as an interface is as much about usability as it is enjoyment and ease. It is the writer’s responsibility to make content compelling and interesting and the typography’s responsibility to make the act of reading feel effortless and enjoyable.

Our blog’s interface is text. The goals of that interface are to:

reduce mental effort when reading and deciding what to read.

simplify the information on screen to increase its usability.

re-use patterns to minimize learning.

provide feedback to the user to give them confidence in their actions.

create an enjoyable experience.

In The Elements of Typographic Style, Robert Bringhurst describes typography as a system based on “the structure and scale of the human body,” particularly the eye, hand, forearm, and of course, the mind.

Just as Bringhurst applies the idea of designing with human behavior and physical attributes in mind, we see that practice equally applied in software and hardware design. Typography should take a form that is natural and beneficial for people to interact with.

After making some updates to our blog, it was interesting to see that many of the considerations made in typographic changes are similar to the considerations we make when designing software.

Here’s a screenshot of the updated designs:

The changes we’ve made are centered around one of the jobs-to-be-done by the blog, which is to provide interesting information to the reader. Because of this, one of our goals is to present concise information in a way that allows the reader to make quick and informed assessments of what they’d like to read.

The header elements (our logo and the blog title) have been grouped to stand out from the list of articles. This makes the page easier to scan for content. The title of an article is now emphasized over the date to present the most important information first. This also allows the link to get to the article to be front and center.

To help the reader create accurate groupings of information, we increased the white space around each article listing and added simple borders to enclose the information related to each article. This prevents the information from bleeding together and causing confusion.

To the individual articles, we decreased the line length of the paragraphs and added more distinct styles to the heading tags to emphasize hierarchy.

Using line length as an example, let’s look more closely at the the impact of typography on user experience. Bringhurst advises that
================================================================================
Rank = 26; Score = 5242880.0
<|begin_of_text|>[This post is by Romain Guy and Chet Haase, Android engineers who have been known to collaborate on the subject of graphics, UIs, and animation. You can read more from them on their blogs at curious-creature.org and graphics-geek.blogspot.com. — Tim Bray]

Earlier this year, Android 3.0 launched with a new 2D rendering pipeline designed to support hardware acceleration on tablets. With this new pipeline, all drawing operations performed by the UI toolkit are carried out using the GPU.

You’ll be happy to hear that Android 4.0, Ice Cream Sandwich, brings an improved version of the hardware-accelerated 2D rendering pipeline to phones, starting with Galaxy Nexus.

Enabling hardware acceleration

In Android 4.0 (API level 14), hardware acceleration, for the first time, is on by default for all applications. For applications at lower API levels, you can turn it on by adding android:hardwareAccelerated="true" to the <application> tag in your AndroidManifest.xml.

To learn more about the hardware accelerated 2D rendering pipeline visit the official Android developer guide. This guide explains how to control hardware acceleration at various levels, offers several performance tips and tricks and describes in details the new drawing model.

I also encourage you to watch the Android Hardware Accelerated Rendering talk that we gave at Google I/O 2011.

Introducing TextureView

Applications that need to display OpenGL or video content rely today on a special type of UI element called SurfaceView. This widget works by creating a new window placed behind your application’s window. It then punches a hole through your application’s window to reveal the new window. While this approach is very efficient, since the content of the new window can be refreshed without redrawing the application’s window, it suffers from several important limitations.

Because a SurfaceView’s content does not live in the application’s window, it cannot be transformed (moved, scaled, rotated) efficiently. This makes it difficult to use a SurfaceView inside a ListView or a ScrollView. SurfaceView also cannot interact properly with some features of the UI toolkit such as fading edges or View.setAlpha().

To solve these problems, Android 4.0 introduces a new widget called TextureView that relies on the hardware accelerated 2D rendering pipeline and SurfaceTexture. TextureView offers the same capabilities as SurfaceView but, unlike SurfaceView, behaves as a regular view. You can for instance use a TextureView to display an OpenGL scene or a video stream
================================================================================
Rank = 27; Score = 5242880.0
<|begin_of_text|>Bitcoin BlockNotary Used for Payment Processor Pay-Me

BlockNotary is a new app that uses Tierion, and it is now used for one of Russia’s largest mobile payment processors. Pay-Me is a Russian mobile payments processor with over 30,000 installed devices, now backed by the power of the Bitcoin blockchain.

In the past, new customers would have to register via interview that was conducted in person with a customer service rep, due to Russian criminals stealing credit cards and proceeding to use mobile PoS terminals to launder money.

“Recording a video of yourself during the registration process helps prevent fraud. As CEO, I’d like to make it as difficult as possible for criminals to use Pay-Me to conduct illegal activities. – Pay-Me CEO, Vladimir Kanin.”

Blocknotary has done away with this, replacing it with a video interview that can be conducted through a mobile application, where Tierion (through Blocknotarys app) collects the data to create verifiable record of the process which is then stored on the blockchain. This greatly accelerates the process, by making the whole verification steps much smoother, allowing the customer to have their PoS device shipped in a timelier manner. A video of the entire process (takes literally around a minute from start to finish) is seen on Tierions blog, linked here.

Blocknotary’s Video Interview application lets customers verify their identity and conduct an interview from their location. The customer’s statement of intentions and the information needed for a background check are collected. This makes the onboarding process smoother for new customers and allows Pay-Me to ship a PoS device before a background check is complete. Additionally, Pay-me gets a tamperproof record of the entire process that can be verified using blockchain. This record can lower Pay-Me’s legal liability and be useful to police that are investigating fraudulent activity.

Tierion is a “cloud platform backed by the power of the blockchain”, allowing users to easily collect and store data. Once data is acquired, it is then anchored to the blockchain to ensure authenticity, and the user is sent a blockchain receipt, allowing the user to verify the data without relying on a trusted third party.

Will other financial institutions look to Bitcoin when it comes to screening new customers? What do you think? Let us know in the comments below!<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 28; Score = 5210112.0
<|begin_of_text|>If you’re thinking about getting pastel hair, you should consider the pros and cons first. Just kidding. Pastel hair is awesome! If you’re torn, do it! If you’re not sure if pastel hair would suit you, do it! You’ll be glad you did. If you want to know how to get pastel hair, keep reading!

Below are all the products you’ll need. They’re clickable, so check them out. The white toner isn’t mandatory; you only absolutely need the bleaching kit, the conditioner, and your choice of hair dye. In addition to this, you’ll need some gloves, a shower cap, and a mixing bowl. The whole process should end up costing under 25$.

[metaslider id=1039]

Step 1: How To Bleach Your Hair

The first step is bleaching your hair. If you have anything but light blonde hair, you absolutely need to bleach it if you want the pastel color to actually be pastel. If you have already blonde hair, skip this step. When choosing your bleaching product, it’s important to go for 30 volume. Anything under that is too weak for achieving pastel hair, and anything over that is too strong for safety — bleach can fry your hair, so be very careful! I recommend this nifty Manic Panic bleaching kit, which also contains gloves, a brush, and a mixing bowl. Tip: If you have very thick or long hair, you WILL need 2 boxes! It ends up going a bit over budget, but it’s well worth it! The first time I bleached my hair, I had to pause the job mid-bleaching and ask my friend to kindly go buy another box for me. Get two if you’re unsure!

Make sure your hair hasn’t been washed for a few days before bleaching. Bleach is harsh on your scalp, and the oils will help protect it.

If it’s your first time, have someone else help you and bleach your hair for you. Bleaching hair is tricky. Be safe!

Follow the instructions on the packaging. A strand test is a very good idea!

If your hair still isn’t light enough after bleaching, consider a white toner. If you want pastel blue hair, your hair has to be a very white blonde. Any yellow in your hair and the blue dye will turn green!

Step 2: How to Get Pastel Hair

There is no “pastel hair dye
================================================================================
Rank = 29; Score = 4947968.0
<|begin_of_text|>Completely Useless Fun Project: Parts Of The Compiler

If you have done C/C++ or Objective-C work on MacOS (There is a project called GNUStep that allows you to run Obj-C on Linux), you may have heard of Clang and LLVM, or maybe Clang/LLVM. It may have confused you because there are two names for a seemingly single piece of software. It may confuse you further to point out that these are two different things, but two pieces to the same puzzle.

What Clang and LLVM are, are the pieces of a compiler. A compiler is really just a single group of processes that takes in some source code and outputs some other code. This output code could be Assembly code, Java ByteCode, hell even Javascript. Like all good programming problems, compiler construction can be broken down into various parts.

First, a history

It may horrify you to know that computer programs weren’t always written in english. For many decades, Programs were written by hand in assembly language. It wasn’t until 1952 that totally bad ass Rear Admiral Grace Hopper built the first compiler and coined the term (she also coined the term debugging after pulling a literal moth out of her computer’s circuits).

This first compiler wasn’t much of anything. It really just operated as more of a linker or a loader than what our modern compilers are. The first real modern compiler was introduced in 1957 by John W Backus and his team at IBM for Fortran.

And compiler design hasn’t changed all that much since then.

Design of the modern Compiler

The Fortran compiler was built in an era where computers were insanely expensive, huge and slow. In order for Fortran to compete with hand coding assembly, it needed to be fast, at least as fast as hand written code. To do this, the compiler was broken down into two parts: the frontend and the backend (that is what Clang and LLVM are, the frontend and backend respectively). This made it easier to apply transformations and optimizations independently.

The phases of the compiler

At the highest level, the frontend takes the source code and outputs what is called an Intermediate Representation or IR. This may look like assembly code, all though it doesn’t have to, but it isn’t. This code is how compilers internally represent the source code. IRs are used so that that it can be optimized and translated much easier. They also must be designed in such away that the intent of the original source code is preserved and be independent of any source or target
================================================================================
Rank = 30; Score = 4915200.0
<|begin_of_text|>Image copyright Getty Images Image caption Disney's Magic Kingdom is the world's most visited theme park, with more than 17 million visitors in 2012.

Disney has raised the price of a one-day ticket to its Magic Kingdom theme park in Orlando, Florida by $4 to $99 (£59.50).

Magic Kingdom is the world's most visited theme park, with more than 17 million visitors in 2012.

Ticket prices at other parks within Walt Disney World, including Epcot and Hollywood Studios, also jumped by $4.

The hike comes after Disney reported surging revenues from its theme park businesses.

In 2013, Disney made $671m from its theme parks - a 16% increase from the same period in 2012.

Overall, its parks brought in $3.6bn in revenue in 2013, a 6% increase from 2012, according to the firm's most recent financial filings.

The boost in revenue was "primarily due to increased guest spending at our domestic parks and resorts, which reflected higher average ticket prices and food, beverage and merchandise spending," according to Disney.

Disney said that the majority of visitors to its parks buy tickets for multiple days.

The company has theme parks throughout the world, with locations in Paris, Tokyo, and Hong Kong.

According to a report from the Themed Entertainment Association and technology company AECOM, Disney's theme parks globally had more than 126 million visitors in 2012, the most recent year for which statistics are available.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 31; Score = 4849664.0
<|begin_of_text|>Software Transactional Memory is a concept which offers many advantages when writing concurrent code. Most importantly, it removes the burden of writing correct multi-threaded code from the programmer, allowing her to focus more on the logic of the problem domain. STM will automatically take care of correctness. However, this comes with a cost – the bookkeeping. Tracking every access to a transactional field in order to properly detect and resolve conflicts introduces a cost penalty. But, at the same time, it also introduces the possibility to create and expose to the programmer some very interesting new semantics.

Shielded has for some time already contained one such semantic addition – the Shield.Conditional static method. With it one may define a transaction which should run whenever a certain condition is satisfied by the current, committed “state of the world”. By virtue of its bookkeeping, Shielded can determine the exact fields your condition depends on. This way it easily knows, when another transaction commits into one of those fields, that your condition might have changed, and will re-evaluate it. (It will even track your condition for any changes in its access pattern between calls. This is important since, due to boolean expressions’ short-circuit evaluation, it is very easy for a condition to change its access pattern, and that must be taken into account.)

This feature was inspired by the Haskell STM’s “retry” command, but it does not go as far as that. The Haskell “retry” can be called anywhere in a transaction, and it will block it until any of the fields, that it has accessed before calling retry, changes. Shielded does not do any such thing, it is a deliberate design decision to not include any threading or blocking constructs into the library. The idea is to allow Shielded to easily be combined with any such construct, depending on what the programmer wants to use, and not lock the programmer into a decision made by this library. And so, the Conditional does not even try executing straight away, but merely subscribes a transaction for future execution.

The power of Haskell’s retry and orElse can be achieved by using Reactive Extensions. Combining Shielded with Rx.NET should not be a difficult task – create an IObservable by creating a Conditional which will call methods on an IObserver (tip – make such calls to IObserver as SideEffects of the Conditional transaction, otherwise you might be triggering observers repeatedly, and with non-committed data!). Once you have such an IObservable, the powerful Rx.NET library allows you to easily create very complex time-based logic,
================================================================================
Rank = 32; Score = 4816896.0
<|begin_of_text|>The issue is particularly nettlesome with individuals who have transitioned from male to female because of the belief that they might still have a physical advantage until their hormone therapy — which not every transgender person chooses to have — is complete.

“When you start talking about transgender athletes, a male-to-female individual, we want to ensure that that is truly a decision that is permanent,” said Bobby Cox, the commissioner of the Indiana High School Athletic Association. “It is not a decision that, ‘I just decided today that I am going to be a girl and I am going to go play on a girls’ team’ and perhaps, disadvantage those kids that are on the team and imbalance the competition.

“And as we progress down this path,’’ he added, “and as we spend more time and energy with advocacy groups and medical professionals in this area, I think that there will be additional amendments to our policies.”

There is no reliable data on the number of transgender high school athletes. Only about 0.6 percent of the adult population identifies as transgender, according to federal data from 2016. Researchers from the Williams Institute, who conducted the study, reported that 0.56 percent of adults in Indiana reported identifying as transgender.

In Texas, 0.66 percent of adults said they identify as transgender, the fifth-highest percentage in the country (behind Hawaii, California, New Mexico and Georgia). Still, transgender children are considered an at-risk minority outside of sports. According to The New England Journal of Medicine, the rate of suicide attempts among transgender people is 40 percent, compared to 4.6 percent among those who are not transgender.

Relatively new policies are already being massaged and amended. In July, the Indiana state association voted unanimously to adjust its surgery requirement. It now no longer requires transgender students who are transitioning from female to male to have sex reassignment surgery in order to compete in the gender with which they identify. But that stipulation is still in place for someone transitioning from male to female.

The group asserts it is acting in the name of fairness, but transgender rights activists accuse members of simply not wanting transgender people to participate, out of fear that those athletes will have an unfair advantage.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 33; Score = 4816896.0
<|begin_of_text|>Throughout my software development career, I’ve seen my fair share of debates over how databases should be developed. And like most disagreements over technical pedantry, the participants are generally well-intentioned but grossly inexperienced. So much so that it wouldn’t matter if they use foreign keys or not. Though “elegant”, their software will become an unmaintainable mess after a few short years. But regardless, one counter-argument that constantly rears its ugly head goes something like, “but what if we have to change it later?”

In other debates, that question can be quickly rebutted with “uh, we just change it then,” but when the discussion is on databases, it actually holds water. It’s as if the database is a marble statue that, once shipped to production, should never be changed... and if it must, there had better be a damn good reason.

Let’s keep in mind that the whole point of developing software – that is, spending large sums of money paying developers’ salaries to build and maintain applications – is to have something that can change according to business need. When developers are unable, afraid, or unwilling to change the applications they develop, they make a very compelling case for being replaced by SAP.

Databases are Different

Databases are indeed different, especially when juxtaposed with what application developers are most familiar with: application code.

Generally speaking, application code lives in a source control system and is a set of instructions that tell that tell the machine to do something. Whether it’s compiled or interpreted, or executed on a real or virtual machine, application code does stuff.

As a developer, when you’re satisfied that it’s doing the right stuff, the release management process kicks in to promote that code through acceptance testing, quality testing, other testing, and finally production. And all along the way, the code should not change. If a bug is found, then it’s fixed in source control and sent right back through the same process.

Databases, on the other hand, live on the production database server. There can certainly be other instances – development, testing, staging – but the only one that really matters is production. And these databases don’t actually do stuff, they’re merely modified and queried by other applications with a Structured Query Language. And unlike application code, databases (or at least, their completely integrated data) do change after deployment – that’s kind of their whole point.

Database Changes Done Wrong

Unlike application code, you can’t exactly drop a bunch of files in a
================================================================================
Rank = 34; Score = 4784128.0
<|begin_of_text|>There has been much talk about modularity recently. Fedora even has a working group on this topic. Modularity is such a generic term that it is a bit hard to figure out what this is all about, but the wiki page gives some hints: …base module… …docker image… …reduced dependencies… …tooling…

A very high-level reading would be that this is about defining tools and processes to deliver software in modules, which can be larger units than packages, and e.g. come in the form of container images.

Another reading would be that this is an effort to come to grips with the changing role of distributions in a world were containers are the dominant way to run and deploy applications.

Modularity on the desktop

In this post, I want to look at how this modularity effort looks from the desktop perspective, and what we are doing in this area.

Our main goal for a while has been to make it easier to get desktop applications from application developers to users.

In the traditional Linux distribution world, this is a total nightmare: Once you’ve written your application, you need to either learn how to create an Ubuntu.deb, an Arch.pkg, a Fedora.rpm, to name just a few, and then follow lengthy processes to get your packages accepted into these distributions.

And once you’ve done all that, you will get bug reports that your application is broken with the one or other version of one of your dependencies. If not right away, then a few months down the road when the distributions move on to the latest and greatest releases. So, keeping those packages working requires the constant attention of a package maintainer.

To improve on this sad state of affairs, we need to make applications much less dependent on the OS they run on, and on the libraries that happen to come with the OS.

At this point, you might say: Aha, you want to bundle dependencies! Yes, but read on. Bundling is a bad word in the traditional distribution world. It implies duplication, since multiple applications may bundle the same library. And having multiple copies of the same library (possibly different versions, too), makes it harder to ensure that bug and security fixes get applied to all the copies. Not to mention that all the duplication consumes bandwidth when you have to download it all.

Bundling everything with the application certainly has its drawbacks. But there are several things we can do to preserve most of the benefits of bundling, while avoiding the worst of the problems.

One takeaway from the modularity discussions mentioned
================================================================================
Rank = 35; Score = 4620288.0
<|begin_of_text|>Pantheon, a WordPress and Drupal hosting service with a strong lineup of features for developers, today announced that it has raised a $29 million Series C round. Investors in this round include previous investors Foundry Group, OpenView Investment Partners, and Scale Venture Partners, as well as new investor Industry Ventures, which put $8.5 million into this round.

This new round follows Pantheon’s $21.5 million Series B round in 2014 and brings the company’s total funding to $57 million.

As Pantheon CEO and co-founder Zack Rosen told me, the company wants to help build the foundational technology for the web and eventually get to the point where it powers 30 percent of all sites. For now, though, he’s happy to simply make progress to getting to 1 percent, which he thinks the company will achieve in a few years. To help make that happen, the company plans to launch a migration toolkit next week that will allow website owners to more easily bring their existing site to the company’s platform.

As Rosen told me, Pantheon now hosts 150,000 sites and while the company doesn’t release customer numbers, Rosen says that the company is seeing customer growth of over 100 percent year-over-year. To host all of this, Pantheon currently has 1.2 million containers in production.

On group of users the company is especially focussing on its agencies. It now has partnered with 2,500 agencies and 50 resellers.

As Rosen told me, the team decided to treat this round of funding like its last one. “We control our burn and are very deliberate in how we deploy our funding,” he added. It’s no secret that raising funding is getting harder and Rosen noted that VCs now want to see more numbers and ask tougher questions before they commit to a round. He does see a positive side of this, too, though. “It’s bad for the industry when VCs don’t ask the kind of questions they should be asking and companies that should be funded get funded — and then down the road they don’t have great results,” he said. “That makes it harder for everyone else.”

The company wants to use this new round of funding to build out its product and make it easier for developers to build their sites on its platform.

As part of today’s funding announcement, the company also announced that it has hired Niall Hayes, who previously led technology at KIXEYE and ran the CityVille studio at Zynga, as its VP
================================================================================
Rank = 36; Score = 4587520.0
<|begin_of_text|>January 2017 An updated post is available: Introducing the MEAN and MERN stacks.

By Valeri Karpov, Kernel Tools engineer at MongoDB and and co-founder of the Ascot Project.

A few weeks ago, a friend of mine asked me for help with PostgreSQL. As someone who’s been blissfully SQL-­free for a year, I was quite curious to find out why he wasn’t just using MongoDB instead. It turns out that he thinks MongoDB is too difficult to use for a quick weekend hack, and this couldn’t be farther from the truth. I just finished my second 24 hour hackathon using Mongo and NodeJS (the FinTech Hackathon co­sponsored by 10gen) and can confidently say that there is no reason to use anything else for your next hackathon or REST API hack.

First of all, there are huge advantages to using a uniform language throughout your stack. My team uses a set of tools that we affectionately call the MEAN stack:­ MongoDB, ExpressJS, AngularJS, and Node.js. By coding with Javascript throughout, we are able to realize performance gains in both the software itself and in the productivity of our developers. With MongoDB, we can store our documents in a JSON-­like format, write JSON queries on our ExpressJS and NodeJS based server, and seamlessly pass JSON documents to our AngularJS frontend. Debugging and database administration become a lot easier when the objects stored in your database are essentially identical to the objects your client Javascript sees. Even better, somebody working on the client side can easily understand the server side code and database queries; using the same syntax and objects the whole way through frees you from having to consider multiple sets of language best practices and reduces the barrier to entry for understanding your codebase. This is especially important in a hackathon setting: the team may not have much experience working together, and with such little time to integrate all the pieces of your project, anything that makes the development process easier is gold.

Another big reason to go with MongoDB is that you can use it in the same way you would a MySQL database (at least at a high level). My team likes to describe MongoDB as a “gateway drug” for NoSQL databases because it is so easy to make the transition from SQL to MongoDB. I wish someone had told me this when I first starting looking into NoSQL databases, because it would have saved me a lot of headaches. Like many people, I was under the impression that CouchDB would be easier to
================================================================================
Rank = 37; Score = 4554752.0
<|begin_of_text|>Space Invaders

//Detect if the browser supports DeviceMotionEvent if (window.DeviceMotionEvent!= undefined) { //ondevicemotion is fired when iOS device detects motion window.ondevicemotion = function(e) { //ax is the movement on the x axis. //This motion is used to move the ship in the game ax = event.accelerationIncludingGravity.x * 5; ay = event.accelerationIncludingGravity.y * 5; //Status 0 is start, 1 is left, 2 is right, 3 is stay if(status == 0){ //initial condition status = 3; //stay socket.emit('spaceChange', {'ax': 3}); statusmsg = 'Waiting for movement'; } if(ax > 14 && status!= 2){ //move right on device status = 2; socket.emit('spaceChange', {'ax': 2}); statusmsg = 'Moving ship right'; } if(ax < -14 && status!= 1){ //move left on device status = 1; socket.emit('spaceChange', {'ax': 1}); statusmsg = 'Moving ship left'; } if(ax > -14 && ax < 14 && status!= 3){ //device held steady status = 3; socket.emit('spaceChange', {'ax': 3}); statusmsg = 'Ship held steady'; }

Current space invaders game uses only X axis movement on the iOS device. However, all three dimension values are available on iOS Safari ondevicemotion method. It is technically possible to do much more with the iOS device.

-- webdigi

The guys over at WebDigi, a London-based web application development company, has released a demo ofSpaceship Pilot, a pretty amazing application that uses iOS devices as the controller for a web browser-based game on your desktop or laptop. No application is required on the iOS device, you simply scan the QR code with the device and the controller code reads the accelerometer values and sends the movements to the node.js server.Here are the key parts of the iOS device controller code:Only changes in direction on the iOS device are pushed to the node.js server, reducing the amount of data pushed between phone and server. The node.js server is coupled with Socket.IO, making it easier to deliver data in realtime using most web browsers. Since Socket.IO code is run on both the server and client sides, pairing it with node.js allows accelerometer values to be pushed instantly from mobile safari to your browser.After testing (read: playing
================================================================================
Rank = 38; Score = 4521984.0
<|begin_of_text|>Double-Spend Protection:

Double-spend protection is the main reason blockchains exist in the first place. In technical terms this means that two valid transactions which spend the same transaction output (UTXO) will conflict and only one can be confirmed in the network. Account based languages (for example Ethereum) that allow for spending the same amount from the same address multiple times, usually have other means to prevent double spending.

Multisig:

Multisig is a very old concept and can be compared to a shared checkbook with multiple required signers. A multisig transaction allows to enforce arbitrary joint signature rules. COMIT uses 2 out of 2 multisig transactions for which both signers have to sign a transaction to become valid and accepted by the network. Multi-signature transactions are a requirement for Payment Channels.

Time-Locks:

A timelock is a simple requirement for funds to be locked up until a future date. Blockchains are found to have 2 different kind of time-locks: relative and absolute time-locks. Absolute time-locks will lock a transaction output until a fixed time in the future. Relative time-locks will lock a transaction output relative to the time the transaction was confirmed. Time-locks are a requirement for trustless Payment Channels and relative time-locks are recommended as they allow for indefinitely open Payment Channels.

Hash function:

To be able to route across multiple blockchains, we need the same hash function to be able available in the smart contracting language of each blockchain. A standard hash function like the SHA256 hash function is usually available and is perfectly suitable for this purpose.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 39; Score = 4489216.0
<|begin_of_text|>In the world of elearning, microlearning has become a buzzword. It’s considered a powerful new approach to training the workforce. With the average attention span in North America dropping from 12 seconds in 2000 to 8 seconds in 2015, the demand for shorter, more engaging training is higher than ever!

In this post, our goal is to cover the basics and leave you with an understanding of: What Microlearning is and the benefits it can provide. Throughout this post, we’ll try to give you examples of how to use microlearning in your own training programs.

What is Microlearning?

Microlearning is focused learning that is delivered in bite sized chunks. Since this method of learning provides small bits of knowledge at a time, it’s best used for delivering information that learners need to retain.

Microlearning can be achieved using a number of different delivery methods: emails, online posts and short multimedia videos, are all examples of different ways you can deliver training that is designed for your learners to retain new knowledge and achieve their educational goals.

Examples of Microlearning:

– Watching video tutorials on Youtube

– Receiving small bits of education via email: like Word of the Day from Dictionary.com

– Online learning programs like Duolingo or Lynda

What are the benefits that Microlearning can provide?

1. Avoid the risk of overwhelming learners

Microlearning allows learners to move at their own pace, giving them the ability to go back and review complex concepts as often as needed. Since new knowledge is delivered in smaller chunks, learners avoid the risk of being overwhelmed by too much information at once.

2. Create on-the-go training that can be accessed anywhere, at anytime.

Microlearning can be achieved using a number of different delivery methods. Email, online posts, videos, even tweets because of this training can usually be accessed across multiple devices, making it available on-the-go. Learners can access and review training materials while doing everyday tasks like: waiting for the bus, sitting on the train, or even riding the elevator!

3. Help learners better retain new knowledge

Traditional classroom training often provides little to no long term takeaways for learners. The Wall Street Journal recently reported that 90 percent of new skills are lost within a year of training! Microlearning breaks new knowledge down into short chunks making learning easier to digest, understand, and apply on the job.

Different ways to use Microlearning:

– Health and Safety training

– Learning new software

– Business Processes and Procedures

Microlearning yields many benefits for organizations looking
================================================================================
Rank = 40; Score = 4456448.0
<|begin_of_text|>Tomorrow (November 7th) we’ll be hosting a testday for the Server Product. Members of the Server Working Group as well as QA will be available on freenode in the #fedora-test-day channel to help out with testing. The focus of the testday will be on several facets of the server product:

At this point you might be asking, “What do each of those things do and how would I benefit from testing them?” I’ll try to give a quick explanation of what each of those features are and why they’re important to the Server Product.

FreeIPA

FreeIPA provides your Linux network with its own Domain Controller capable of managing users, groups, DNS, certificates and single-sign-on capabilities. Taken from the FreeIPA website:

FreeIPA is an integrated Identity and Authentication solution for Linux/UNIX networked environments. A FreeIPA server provides centralized authentication, authorization and account information by storing data about user, groups, hosts and other objects necessary to manage the security aspects of a network of computers.

Cockpit

Cockpit is a web based tool for managing and monitoring servers. It takes a lot of the pain out of administering several servers allowing you to do it from one, easy to use, central location. It takes a lot of the pain out of managing all your services, users, containers and pretty much anything your server might be doing.

OpenLMI

OpenLMI is a project aimed at creating a standardized API based on DMTF-CIM for managing your Linux systems. From the their site:

The OpenLMI project provides a common infrastructure for the management of Linux systems. Capabilities include configuration, management and monitoring of hardware, operating systems, and system services. OpenLMI includes a set of services that can be accessed both locally and remotely, multiple language bindings, standard APIs, and standard scripting interfaces.

Rolekit

Rolekit is the tool used to enable the easy deployment of different server roles. It makes it a snap to deploy different kinds of servers, such as web, mail, domain controllers, etc. (referred to as “roles”).

Conclusion

Suffice it to say, Server offers a lot of new and great functionality. So come by for the test day and take some time to get familiar with it’s offerings and make it an awesome release! More information can be found on the wiki – and you can always drop by #fedora-server or #fedora-qa if you have any questions or want to get involved!<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 41; Score = 4390912.0
<|begin_of_text|>For almost three years, millions of servers and smaller devices running Linux have been vulnerable to attacks that allow an unprivileged app or user to gain nearly unfettered root access. Major Linux distributors are expected to fix the privilege escalation bug this week, but the difficulty of releasing updates for Android handsets and embedded devices means many people may remain susceptible for months or years.

The flaw, which was introduced into the Linux kernel in version 3.8 released in early 2013, resides in the OS keyring. The facility allows apps to store encryption keys, authentication tokens, and other sensitive security data inside the kernel while remaining in a form that can't be accessed by other apps. According to a blog post published Tuesday, researchers from security firm Perception Point discovered and privately reported the bug to Linux kernel maintainers. To demonstrate the risk the bug posed, the researchers also developed a proof-of-concept exploit that replaces a keyring object stored in memory with code that's executed by the kernel.

The vulnerability is notable because it's exploitable in a wide array of settings. On servers, people with local access can exploit it to achieve complete root access. On smartphones running Android versions KitKat and later, it can allow a malicious app to break out of the normal security sandbox to gain control of underlying OS functions. It can also be exploited on devices and appliances running embedded versions of Linux. While security mitigations such as supervisor mode access prevention and supervisor mode execution protection are available for many servers, and security enhanced Linux built into Android can make exploits harder, there are still ways to bypass those protections.

Update, Jan. 20, 1:48 PST: In a post published a day after this post went live, Google said company researchers don't believe any Android devices are vulnerable to exploits by third-party applications. It also said researchers believe that the number of Android devices affected is "significantly smaller than initially reported." Google will nonetheless issue an update in March that patches the vulnerability.

"As of the date of disclosure, this vulnerability has implications for approximately tens of millions of Linux PCs and servers, and 66 percent of all Android devices (phones/tablets)," Perception Point researchers wrote. "While neither us nor the Kernel security team have observed any exploit targeting this vulnerability in the wild, we recommend that security teams examine potentially affected devices and implement patches as soon as possible."

While malware distributors have focused most of their resources over the years on infecting computers running Microsoft Windows, they have put increased focus on attacking competing OSes. In 2014,
================================================================================
Rank = 42; Score = 4390912.0
<|begin_of_text|>The Scream Remake Made The Weirdest Decision Possible By Nick Venable Random Article Blend Scream Scream!”

The news comes from Ghost Face creator R.J. Tolbert, who spoke with

Please note that I had been in communication with TWC regarding this and they have informed me that, as of now and during the initial launch, that Ghost Face is not involved in the new format. They also indicated that because of this direction, it does not mean, that Ghost Face will not be involved at a later date. So, there is still a question, to the question. However, as of now, there is no involvement.”

You gotta love anyone who says there is still a question to a question. This is pretty surprising news, given I can’t possibly correlate any other image with the concept of Scream than the Ghost Face mask. (Nor the Scary Movie films.) Sure, I can picture Jaime Kennedy in turmoil, but I don’t want that to get showcased in the MTV series either. Tolbert wanted to make it clear to everyone that he feels the same way about the connection between the killers’ mask choice and the film series.

We [Fun World] believe Scream is Ghost Face and Ghost Face is Scream. However, while Ghost Face is owned by Fun World, the Scream motion picture franchise is owned by The Weinstein Company, and it is their option to film a movie or TV show without Ghost Face.”

The

Sure, there’s still a chance that Ghost Face will make his presence known at another point in the series’ existence, but it has to actually last first. And what are the chances of that happening if they don’t even have the core franchise image to market it with? Every so often, a film sequel gets made despite having zilch to do with the original film. It occurs less often on TV, but that’s exactly what’s happening with MTV’s upcoming series, based on the highly lucrative film franchise of the same name. The show’s creative team will reportedly be leaving the Ghost Face mask out of it altogether, which basically means this series will be inspired by murdered teenagers. News about dead teens always makes me think, “That’s so!”The news comes from Ghost Face creator R.J. Tolbert, who spoke with GhostFace.co recently about the subject of the copyrighted character appearing in the series, and here’s what he had to say.You gotta love anyone who says there is still a question to a question. This is pretty surprising news, given I can’t
================================================================================
Rank = 43; Score = 4390912.0
<|begin_of_text|>MIDI to Audio with frozen tracks – No Flatten

Have you ever wanted to turn your MIDI track to Audio but keep the MIDI? Well, I use this quick tip all the time. I discovered it by mistake when editing years back, and find myself doing it all the time. What you need to do is first Freeze your MIDI track. Once it is frozen you can select the MIDI Clip and drag it into an Audio Track by holding Ctrl or CMD while you move it. This will create a copy of the “frozen” audio in the Audio Track, but the MIDI will still be there. You can then Unfreeze the MIDI track.

Copy and Paste Time

When you are writing a track there are times that you want to copy a whole section of the music or move the Verse after the Chorus, or you name it. This can be really tedious without knowing this workflow hack. The key is the Cut Time, Paste Time, and Duplicate Time option in Ableton Live.

All you need to do is select an area in Arrangement View. This will be the area that will be duplicated or cut. Then navigate up to the Edit –> Duplicate Time. This will duplicate everything during that time period of the track. I will usually then pick, Edit –> Cut Time, select where I want this section to go and then, Edit –> Paste Time. This way I can duplicate the whole verse or chorus and place it later on in the track as I am building the composition.

Drag in Older Versions

Let’s say you are working on a track. You save multiple versions as you go, this is just a damn good idea. You had a baseline and started to rewrite it. After a while, you realize that you liked the old baseline, but have done a lot of other things along the way. You can actually drag a track from one of the older versions. This makes it easy to revert back to a past part.

To do this I usually navigate to the Current Project in my Browser. This shows all other versions you have saved if it is in the same project folder. If not, or you want to bring in another track from another song you can just navigate there with your browser. Next up you click the triangle next to the Live Set to open it up. You will then see all the tracks in the Live Set. Click the track you want and drag it into Live.

This lets you easily revert to older versions and save a lot of time.

Copy Value to Sibling

When you are tweaking
================================================================================
Rank = 44; Score = 4292608.0
<|begin_of_text|>Some practical tricks for training recurrent neural networks:

Optimization Setup

Adaptive learning rate. We usually use adaptive optimizers such as Adam (Kingma14) because they can better handle the complex training dynamics of recurrent networks that plain gradient descent.

We usually use adaptive optimizers such as Adam (Kingma14) because they can better handle the complex training dynamics of recurrent networks that plain gradient descent. Gradient clipping. Print or plot the gradient norm to see its usual range, then scale down gradients that exceeds this range. This prevents spikes in the gradients to mess up the parameters during training.

Print or plot the gradient norm to see its usual range, then scale down gradients that exceeds this range. This prevents spikes in the gradients to mess up the parameters during training. Normalizing the loss. To get losses of similar magnitude across datasets, you can sum the loss terms along the sequence and divide them by the maximum sequence length. This makes it easier to reuse hyper parameters between experiments. The loss should be averaged across the batch.

To get losses of similar magnitude across datasets, you can sum the loss terms along the sequence and divide them by the maximum sequence length. This makes it easier to reuse hyper parameters between experiments. The loss should be averaged across the batch. Truncated backpropagation. Recurrent networks can have a hard time learning long sequences because of vanishing and noisy gradients. Train on overlapping chunks of about 200 steps instead. You can also gradually increase the chunk length during training. Preserve the hidden state between chunk boundaries.

Recurrent networks can have a hard time learning long sequences because of vanishing and noisy gradients. Train on overlapping chunks of about 200 steps instead. You can also gradually increase the chunk length during training. Preserve the hidden state between chunk boundaries. Long training time. Especially in language modeling, small improvements in loss can make a big difference in the perceived quality of the model. Stop training when the training loss does not improve for multiple epochs or the evaluation loss starts increasing.

Especially in language modeling, small improvements in loss can make a big difference in the perceived quality of the model. Stop training when the training loss does not improve for multiple epochs or the evaluation loss starts increasing. Multi-step loss. When training generative sequence models, there is a trade-off between 1-step losses (teacher forcing) and training longer imagined sequences towards matching the target (Chiappa17). Professor forcing (Goyal17) combines the two but is more involved.

Network Structure

Gated Recurrent Unit. GRU (Cho14
================================================================================
Rank = 45; Score = 4292608.0
<|begin_of_text|>Almost four years ago, I documented a really cool vSAN capability here and here, which demonstrates how to bootstrap a vSAN datastore onto a single ESXi host. This powerful capability, which was by design, enables customers to easily standup new infrastructure including the vCenter Server Appliance (VCSA) in a pure greenfield environment where you only had bare-metal hardware to start with and no existing vCenter Server.

As you can probably guess, I am a huge advocate for this capability and I think it enables some really interesting use cases for being able to quickly and easily stand up a complete vSphere environment without having to rely on an external storage array or playing games with Storage vMotion'ing the VCSA between local VMFS and the vSAN datastore for initial provisioning.

Over time, this vSAN capability has gone mainstream not only from a customer standpoint but also internal to VMware. In fact, the use of this feature has made its way into several VMware implementations including but not limited to VMware Validated Designs (VVD), VxRail, VMware Cloud Foundation (VCF) and even in the upcoming VMware Cloud on AWS. This really goes to show how useful and critical of a feature this has become for standing up brand new VMware infrastructure which runs on top of vSAN. Huge thanks goes out to the original vSAN Architects who had envisioned such use cases and designed vSAN to include this functionality natively within the product and not have to rely or depend on vCenter Server.

So what has changed in the with the new release of vSAN 6.6 (vSphere 6.5d) For starters, this functionality continues to exists, but the Engineers and specifically I would like to call out Christian Dickmann who was the lead architect for what I am about to talk about reached out to me and asked, could we make this even better? For those of you who have walked through the vSAN bootstrap process, although pretty straight forward it still does require quite a few steps to accomplish. Even with Automation, the overall user experience could still be improved and today, this bootstrap process is only available using the CLI or calling into several different remote APIs.

In vSAN 6.6, the vSAN bootstrap process is now natively integrated into the VCSA UI Installer which is referred to as the vSAN Easy Install feature. This means, as part of selecting an ESXi host to deploy the VCSA (for a pure greenfield deployment), you will now also be able to configure the vSAN
================================================================================
Rank = 46; Score = 4259840.0
<|begin_of_text|>The world is changing.

No – the world as we knew it in IT has changed.

Big Data & Agile are hot topics.

But companies still need to collect, report, and analyze their data. Usually this requires some form of data warehousing or business intelligence system. So how do we do that in the modern IT landscape in a way that allows us to be agile and either deal directly or indirectly with unstructured and semi structured data?

First off, we need to change our evil ways – we can no longer afford to take years to deliver data to the business. We cannot spend months doing detailed analysis to develop use cases and detailed specification documents. Then spend months building enterprise-scale data models only to deploy them and find out the source systems changed and the models have no place to hold the now-relevant data critical to business success.

We have to be more agile than that.

We need a way to do Agile Data Engineering if we ever expect to achieve the goal of Agile Data Warehousing.

The Data Vault System of Business Intelligence provides (among other things) a method and approach to modeling your enterprise data warehouse (EDW) that is agile, flexible, and scalable. This is the first in a series of posts introducing you to the Data Vault, what it is, and how to build one.

Why Data Vault?

Current approaches to modeling a data warehouse include developing 3rd Normal Form (3NF) type models or dimensional star schema models. Now, while there are indeed architects and modelers out there who have been wildly successful using these approaches, there are many more that have failed, and failed big.

Why? Mostly lack of experience, but more so that these approaches have some basic issues that, while resolvable, do require a certain level of engineering expertise that is not readily available in the industry today (and is declining daily).

What are these issues?

In order to collect history in the usual 3NF approach you need to add a timestamp or snapshot date to every table in your model. Not only that but it needs to be in the primary key of the table so that you do not load duplicate rows on any given day. This of course complicates all the cascading foreign key (FK) relationships. In Figure 1 you can see a simple example of a standard Product and Product Category table with Snapshot Dates added to the keys.

Figure 1. Basic model with Snapshot Dates added

Even with this simple model you can start to see the issues. Adding a snapshot date to every PK makes all the keys more complicated.
================================================================================
Rank = 47; Score = 4259840.0
<|begin_of_text|>The growing world of smartphones and the use of instant messengers are playing a bigger role in daily life than ever before, these IM apps are connecting people with each other conveniently and freely in the digital world. Moreover, the use of instant messengers and the user’s adoption of Android devices on a larger scale is creating fear in the minds of the parents. So, the rain of instant messengers, mostly youngsters are involved in the usage of different kinds of instant messengers on the same Android device. Having said that, parents are fearful because of plenty of IM apps on their young kid’s mobile devices; it makes it difficult for parents to monitor multiple instant messengers on a particular Android device. There are a number of instant messengers but the most popular are Facebook Messenger, WhatsApp, Viber, Snapchat, Kik, Line, IMO, and Skype. Monitoring of a particular messenger is not really difficult but spying on multiple instant messengers on Android devices seems to be a problem. It’s because the best instant messenger’s apps work using similar methodology and the actual difference is the user interfaces and having some extra functions of every app. TheOneSpy (TOS) will let you monitor multiple IM apps on Android devices, you will be able to spy on up to 16 instant messaging application with the help of TOS monitoring app. All the instant messaging apps have very secure services; therefore to spy on multiple messengers, we have to apply some tactics to monitor multiple IMs.

How is that possible?

In order to monitor more than one instant messenger, we need rooting of our Android devices. Now the question is, how do we root the devices; It’s actually simple, we can use a software which can easily root almost any device. It’s called “KingoRoute” and is a free application. So, use this application and after the completion of the rooting of your device, you will enable to monitor multiple instant messengers with the help of TOS monitoring application.

Why is the TheOneSpy the best option for spying multiple Messengers in Android devices?

It’s because the TheOneSpy enables you to monitor multiple instant messaging applications at once, having single control panel you can monitor all of the conversation happening on multiple messengers on the single platform with complete and accurate time statistics. TOS is compatible with Android 4.0 up to Android 7.0 Nougat. TOS is reasonably priced, starting from $0.6/day. You might be able to find some discounts if you google it. Following
================================================================================
Rank = 48; Score = 4227072.0
<|begin_of_text|>Without affecting the original signal, we removed all of the outliers, yet introduce new ones. That is the shortcoming of the method, since change is a lot in a small interval(around peaks), this method introduces medians even if there is no outliers. The unwanted signal is closer to original signal, so there is still improvement. Further, this is easy to prevent with another heuristics. We will only look at the signal that behaves regularly or using a sliding window to have a more robust approach. But this example is important to show how a non-linear filter is able to reject all of the outliers without doing anything very complicated. This has due to mainly two reasons. First one is that $l_1$ norm. Instead of using $l_2$, we use $l_1$ as this norm does not change too much when an outlier gets introduced to the signal whereas $l_2$ norm(since it takes the square of the distance) outliers have much more profound effect. We are preventing using $l_1$ norm. Second one is that we are filtering adaptively. Instead of using a mean filter, and average all of the numbers, we only look at the candidates that could be outliers. First, we get convinced those points are outlier, then we attempt to remove them. This makes it hard to reject the regular signal. What about disadvantages? First and foremost, it is nonlinear filtering. There is no going back. You cannot get the original signal that you begin with. Further, we are also applying a threshold which makes it even harder. There are two important parameters, threshold and sliding window which requires some parameter tuning. But generally outliers are quite either signal specific or application specific. Therefore, this may not be a disadvantage.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 49; Score = 4227072.0
<|begin_of_text|>It’s no secret that the installations at Burning Man are highly innovative and creative. The festival in Black Rock, Nevada, which is happening right now, promises to offer an escape from the regular constraints of society. This year, the festival is bigger than ever, featuring high-profile producers like Skrillex and Diplo and an all-new live stream to capture moments of the event. One creative group is taking it one step further by converting an airplane into a nightclub.

The Big Imagination Foundation spent several months turning this Boeing 747 into what has been described as the “largest moving art experience ever made”. The project will stand on the Playa close to other art installations. The inside of the airplane will feature music and art. It will also travel around Black Rock City, pulled by an aircraft tug car.

Inside the airplane, the experience begins once the participants answer the question “What baggage do you need to lose?”. An “Emotional Baggage Tag” is then created along with the boarding pass and passengers are later asked “Where are you going?” The answers to this question are displayed on the walls inside the airplane. The immersive installation is sure to be a psychological and liberating experience for attendants.

For those of us that are wishing we were there, the Big Imagination Foundation is looking to develop a virtual reality experience for all. They are currently crowd-funding through their IndieGogo campaign.

H/T: The Creator\’s Project<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 50; Score = 4227072.0
<|begin_of_text|>In graph like data structures (explicit or implicit) there is a problem of memory safety when you remove a node. You have to clean up all the connections pointing to you. If your graph structure is explicit this is easy, otherwise it can often be annoying to find all pointers pointing to you. A while ago I thought that you could solve that by introducing a two way pointer, which has to be the same size as a normal pointer, but which knows how to disconnect itself on the other side, so that you don’t have to write any clean up code.

The reason why it turned out surprisingly useful is that if both ends of the connection know about each other, I can make them keep the connection alive even if one of them is moved around. So if one of them lives in a std::vector and the vector reallocates, the TwoWayPointer can handle that case and just change the pointer on the other end to point to the new location. That can dramatically simplify code.

One example is that I’m now using it in my signal/slot class which I’ve also included in this blog post. It allowed me to simplify the signal/slot implementation a lot. Compare it to your favourite signal implementation if you want. There is usually a lot of code overhead associated with memory safety. That all went away in this implementation.

Here is the code for the two way pointer:

#pragma once #include <memory> #include <cstddef> template<typename T, typename S> struct TwoWayPointer { private: TwoWayPointer<S, T> * ptr; template<typename, typename> friend struct TwoWayPointer; template<typename T2, typename S2> friend bool operator==(const TwoWayPointer<T2, S2> &, const TwoWayPointer<T2, S2> &); template<typename T2, typename S2> friend bool operator!=(const TwoWayPointer<T2, S2> &, const TwoWayPointer<T2, S2> &); template<typename T2, typename S2> friend bool operator==(const TwoWayPointer<T2, S2> &, std::nullptr_t); template<typename T2, typename S2> friend bool operator!=(const TwoWayPointer<T2, S2> &, std::nullptr_t); template<typename T2, typename S2> friend bool operator==(std::nullptr_t, const TwoWayPointer<T2, S2> &); template<typename T2, typename S2> friend bool operator!=(std::nullptr_t, const TwoWayPointer<T2,
================================================================================
Rank = 51; Score = 4194304.0
<|begin_of_text|>FAQ

Is CLIC Marble made from real marble?

Yes. Our CLIC Marble is made from the highest-quality white marble or black marble. The case features a very thin slice of marble to ensure it is lightweight and doesn’t add bulk to your iPhone.

How much does CLIC Marble weigh?

CLIC Marble for iPhone 6/6s weighs 31 grams (1.1 ounces) — in comparison, CLIC Wooden for iPhone 6/6s weighs 23 grams (0.8 ounces), and an iPhone 6 weighs 136.5 grams (4.8 ounces). CLIC Marble for iPhone 7 and 8 weighs 37 grams (1.3 ounces) — in comparison, CLIC Wooden for iPhone 7 and 8 weighs 28 grams (1 ounce), and an iPhone 7 weighs 138 grams (4.7 ounces). CLIC Marble for iPhone 7 Plus and 8 plus weighs 49 grams (1.7 ounces) — in comparison, CLIC Wooden for iPhone 7 Plus and 8 Plus weighs 35 grams (1.2 ounces), and an iPhone 7 Plus weighs 188 grams (6.6 ounces).

What can I expect from the finish?

Marble is a natural stone formed over thousands of years. Each slice has unique and intricate veins, so the finish will vary from case to case.

Is the CLIC Marble shatter resistant?

Yes. We deliberately used a very thin slice of marble laminated to a layer of fiberglass, giving the marble a degree of flexibility. This makes it easier to take on and off your iPhone and resistant to shattering.

How do I care for my marble?

Although a solid natural stone, marble is in comparison to other stones quite soft and porous. As such, care should be taken to keep your marble looking its best. To clean your marble, use warm water, a soft cloth, and a mild soap such as washing-up liquid. Never use anything acidic or abrasive to clean your marble, including natural solutions vinegar or lemon, which will dull the surface of the marble. Apply marble polish, available from hardware stores, on a regular basis to maintain the stone’s luster.

Is the packaging recyclable?

We’re proud to say that all our packaging is 100% recyclable, so you can enjoy our great products without hurting the planet.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 52; Score = 4194304.0
<|begin_of_text|>InfluxDB : InfluxDB is an open-source time series database written in Go that has been built to work best with metrics, events, and analytics. Using InfluxDB, you can easily store system and application performance data and manage any time series data.

Grafana : Grafana is an open-source, general purpose dashboard that is used for visualizing time series data for Internet infrastructure and application analytics. Grafana supports graphite, influxdb or opentsdb as backends and runs as a web application.

In this tutorial, we will learn how to create and run Grafana and InfluxDB Docker containers in Ubuntu 14.04.

Requirements

A server running Ubuntu-14.04 with Docker installed.

A non-root user with sudo privileges setup on server.

Creating The Dockerfile

First, you will need to create the Docker file to install all requisite software. Create docker file inside your home directory using the following command:

sudo nano Dockerfile

Add the following lines with all requisite software:

FROM ubuntu MAINTAINER Hitesh Jethva (hitjethva@gmail.com) RUN apt-get update && apt-get -y --no-install-recommends install ca-certificates software-properties-common python-django-tagging python-simplejson python-memcache python-ldap python-cairo python-pysqlite2 python-support python-pip gunicorn supervisor nginx-light nodejs git curl openjdk-7-jre build-essential python-dev

Add the following lines to install Grafana, InfluxDB, and do some basic configuration:

WORKDIR /opt RUN curl -s -o /opt/grafana-1.8.1.tar.gz http://grafanarel.s3.amazonaws.com/grafana-1.8.1.tar.gz && curl -s -o /opt/influxdb_latest_amd64.deb http://s3.amazonaws.com/influxdb/influxdb_latest_amd64.deb && mkdir /opt/grafana && tar -xzvf grafana-1.8.1.tar.gz --directory /opt/grafana --strip-components=1 && dpkg -i influxdb_latest_amd64.deb && echo "influxdb soft nofile unlimited" >> /etc/security/limits.conf && echo "influxdb hard nofile unlimited" >> /etc/security/limits.conf

Next, copy some configuration files:

ADD config.js /opt/grafana/config.js ADD nginx.conf /etc/nginx/nginx.conf ADD supervisord.conf /etc/supervisor/conf.d/superv
================================================================================
Rank = 53; Score = 4177920.0
<|begin_of_text|>In everyday life, I'm a web developer. Or, to be precise, I run a business that develops websites for a wide range of clients, from small businesses to large organizations. Every one of these sites comes with a CMS of some sort. Which CMS we use to develop the sites depends on a lot of factors, including what the client wants, the size of the website, and the required functionality. In this article, I'll cover the lessons learned when we developed our open source Bolt content management system.

Keep the customer in mind

Often when working on larger site, you'll work with a small group of people with different job descriptions and skill sets. This overlap between people with different areas of expertise can cause some problems when interests collide:

An editor doesn't want to know about database structures. They don't care if something is XML, JSON, or Mediumtext.

The backend developer shouldn't concern themselves with the exact markup that's used on the website. <b> or <strong>, that's pretty much the same, right?

Front end developers shouldn't need to know about scheduling new articles or whether or not the editor-in-chief has approved a change to an older article.

I strongly believe that there's no single CMS that's well suited to run all websites, and I'm always surprised that there are a lot of web development agencies that use the same CMS for every single project they do. If I were a client of an agency like that, I'd wonder if the CMS I'm getting is really the best fit for my website, or if it just happens to be the CMS they use for all their clients.

Because this is not how we would like to work, we've been looking around for CMSs that fit our workflow and, at the same time, complement each other. This way we have a few options to better help our clients without falling back to the same system every time. For most of the larger projects we develop, we use Drupal. It's a great system with a lot of functionality. Because of this, it's also a rather complex system (Especially for the editors, who eventually have to work in the system on a daily basis). We've done a lot of research to find a system that complements this: A simpler, more lightweight system for sites that don't require the extensive functionality offered by Drupal.

Every system that we've evaluated when looking for this had one or more major downsides. It seems like every single CMS out there is written by a certain kind of person for a certain
================================================================================
Rank = 54; Score = 4161536.0
<|begin_of_text|>That's not all the article says. But since you failed to completely read my own first paragraph, I'm not surprised you didn't get through the article. I'll let you go re-read it. Also, all I'm seeing is still promising. You've yet to present any evidence whatsoever of them being harmful.

Their use in this way has been studied somewhat (again..check the article), but obviously we don't yet know long-term effects for this particular use. But as you quoted, they have been safely used as a pause button in other contexts. There's no reason to think this wouldn't be safe.

We have no reason to believe that medically halting puberty would affect someone mentally. Again, if people who naturally hit puberty late mature mentally the same, why should medical intervention be any different? All they're affecting is the hormones that spur the development of secondary sex characteristics. Your concerns are based in nothing but your own bias.

It is very easy to change your mind based on all we know about different circumstances. You know, the ones where they've been used safely? Here's a crazy thing you keep forgetting. It's the child's choice. Puberty occurring naturally is something terrifying for transgender kids. It's a train they can't stop. Until puberty blockers. If a kid is getting uncomfortable with their body not developing they can be taken off the blockers. It's an easy fix. I'll refer you to the logic problem from earlier. The same train of thought here applies to why something that is easy to reverse is a better course than something that is hard to reverse. But since you're a broken record, I know you're just going to say "we don't know they're easy to reverse," but yeah. We do. Which is how they've been, again, used safely in other contexts.

These kids might be helped in other ways. Or we could help them in a way we know works. Where the only safety concerns about them come from people who are against transgender people in the first place (I tried to resist going through your comment history, but it's not hard to spot an r/Gender_Critical user. Also great to see you're the same asshole who posted that whorephobic pole dancing article).

Your whole last paragraph is just repetition of the same unfounded fears your entire argument is based on. Maybe take a step back and realize while there's actually evidence for the safety of puberty blockers, your entire opinion is based on fear and hate. Why don't you take a second
================================================================================
Rank = 55; Score = 4128768.0
<|begin_of_text|>Image caption The NSA wants to use its quantum computer to break encryption used to protect online communication

The US National Security Agency is building a quantum computer to break the encryption that keeps messages secure, reports the Washington Post.

The NSA project came to light in documents passed to the newspaper by whistle-blower Edward Snowden.

The spying agency hopes to harness the special qualities of quantum computers to speed up its code-cracking efforts.

The NSA is believed to have spent about $80m (£49m) on the project but it has yet to produce a working machine.

If the NSA managed to develop a working quantum computer it would be put to work breaking encryption systems used online and by foreign governments to keep official messages secure, suggest the documents excerpted in the Post.

The quantum computer is being developed under a research programme called Penetrating Hard Targets and is believed to be conducted out of a lab in Maryland.

Processing power

Many research groups around the world are pursuing the goal of creating a working quantum computer but those developed so far have not been able to run the algorithms required to break contemporary encryption systems.

Current computers attempt to crack encryption via many different means but they are limited to generating possible keys to unscramble data one at a time. Using big computers can speed this up but the huge numbers used as keys to lock away data limits the usefulness of this approach.

By contrast, quantum computers exploit properties of matter that, under certain conditions, mean the machine can carry out lots and lots of calculations simultaneously. This makes it practical to try all the possible keys protecting a particular message or stream of data.

The hard part of creating a working quantum computer is keeping enough of its constituent computational elements, called qubits, stable so they can interact and be put to useful work.

The NSA is not believed to have made significant breakthroughs in its work that would put it ahead of research efforts elsewhere in the US and Europe. However, the documents passed to the Post by Edward Snowden suggest the agency's researchers are having some success developing the basic building blocks for the machine.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 56; Score = 4063232.0
<|begin_of_text|>Patients used to pay $10,000 a year for cancer drugs, but now they’re paying $10,000 a month, according to HBO talk show host Bill Maher.

That’s a message circulating on Facebook recently, promoted by the "Bill Maher Fanpage." A reader asked us to check it out.

The quote comes from the Oct. 2 episode of Real Time with Bill Maher, where Maher discussed Turing Pharmaceuticals, the company that jacked up the cost of a life-saving drug from $13.50 to $750 essentially overnight.

"Not that (Turing CEO Martin) Shkreli is unusual for the pharmaceutical industry," Maher said on his show. "Fifteen years ago, cancer drugs cost an average of $10,000 a year. Now it’s $10,000 a month because this cartel owns the U.S. government every bit as much as Mexican drug lords own theirs."

It’s well known that the cost of health care is on the rise, but has the annual cost of cancer drugs really increased so dramatically in just a decade and a half?

Maher’s team sent us links to some news articles about the rising cost of cancer drugs, and we tracked down information that speaks to his claim.

The best information we could nail down is the cost of anticancer drugs at launch, the initial price when the Federal Drug Administration approves a drug.

These are the sticker prices set by the pharmaceutical companies, not the prices patients actually pay out of pocket, which are reduced by insurance payments, patient assistance programs and other discounts. Costs increases, though, are often passed on to consumers.

Maher has a point that cancer drug costs are on the rise. We charted the launch costs of cancer drugs the FDA approved since 1999, using a list compiled by Memorial Sloan Kettering Cancer Center.

The second half of Maher’s statement -- that cancer drugs now cost about $10,000 a month -- is also on solid ground. Of drugs approved in 2015, the monthly average cost is $11,319. And experts in the field often cite that $10,000 figure themselves.

Because many cancer drugs are not administered to patients for a full year, it’s difficult to determine an average annual cost, so the first half of Maher’s statement -- that they cost about $10,000 a year 15 years ago -- is hard to nail down. To his point, though, the monthly cost of a new drug was dramatically lower in 2000 than what it is now at about $4,
================================================================================
Rank = 57; Score = 4030464.0
<|begin_of_text|>You must enter the characters with black color that stand out from the other characters

Message: * A friend wanted you to see this item from WRAL.com: http://wr.al/14Iqb

— Customers that purchased a ticket through Ticketmaster between late 1999 and early 2013 could be eligible for free tickets to a number of events.

An email sent to eligible Ticketmaster customers includes instructions on how to get vouchers for free tickets to selected events as well as discounts on Ticketmaster purchases. Those who bought a ticket through the company between Oct. 21, 1999, and Feb. 27, 2013, are eligible. The vouchers expire in four years.

The vouchers are the result of a class-action lawsuit over ticket fees and other charges.

Frank Manginaro, of Chatham County, is one of about 57 million people who recently received a voucher.

He said he was thrilled until the site started giving him an error message when he attempted to use the voucher code.

"All of a sudden they are giving me excuses saying it doesn't work, you can't buy tickets, try again later," he said. "I was upset about it, and so were my friends because they couldn't even get through."

After trying for two days, Manginaro tried again and found a way to get tickets. He said he ignored the voucher link on the Ticketmaster website and went straight to the band's page.

"I just skipped over it and went straight to the Counting Crows and ordered it through there. It went through with no problem," he said.

According to the settlement, Ticketmaster is required to distribute 42 million dollars in vouchers in the next four years.

Ticketmaster is now part of Beverly Hills-based Live Nation Entertainment Inc.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 58; Score = 3997696.0
<|begin_of_text|>Micro Service Architecture is an architectural concept that aims to decouple a solution by decomposing functionality into discrete services. Think of it as applying many of the principles of SOLID at an architectural level, instead of classes you've got services.

Conceptually speaking MSA is not particularly difficult to grasp but in practice it does raise many questions. How do these services communicate? What about latency between services? How do you test the services? How do you detect and respond to failure? How do you manage deployments when you have a bunch of interdependencies? So lets expand on some of these throughout this post and see if MSA really is worth the effort.

Anatomy of a Micro Service

First things first what actually is a micro service? Well there really isn't a hard and fast definition but from conversations with various people there seems to be a consensus that a micro service is a simple application that sits around the 10-100 LOC mark. Now I realise that line count is an atrocious way to compare implementations so take what you will from that. But they are small, micro even. This means you're not going to find hundreds of tiny services built on top of large frameworks, it's simply not practical. No, simplicity and lightweightyness (not a real word) is the order of the day here. Small frameworks like Sinatra, Webbit, Finagle & Connect do just enough to allow you to wrap your actual code in a thin communication layer.

In terms of a footprint these services will be small, you're potentially going to run a lot of them on the same machine so you don't want to be holding on to memory or resources that you aren't intending to use. Once again simple libraries over large frameworks will win out, you'll also find less of a reliance on 3rd party dependencies.

This decoupling at a service level also offers another interesting option. We've pushed a lot of the old application complexity down to infrastructure level. We are no longer bound to a single stack or language. We can play to the strengths of any stack or language now. It's entirely possible to have a system built out with a myriad of languages and libraries, though as we will touch on later this is a double edged sword.

You're also not going to find any true micro service based architectures that are hosted in an application server, that kinds of defeats the point. To this end micro services self host, they grab a port and listen. This means you'll lose any benefits your typical enterprise application server may bring and your service
================================================================================
Rank = 59; Score = 3981312.0
<|begin_of_text|>If I asked you to name major Java vendors, chances are EMC's VMware subsidiary wouldn't top the list. To most of us, the "VM" in VMware doesn't stand for the Java Virtual Machine; it means the other kind of virtual machine, the kind that lets you run servers and desktops on virtualized OS instances. But if that's your assumption, it may be time to change your thinking.

VMware is reinventing itself. Virtualization may have been the hot topic a few years ago, but its star faded once the OS vendors got into the game. Now cloud computing is the buzzword of the day. Observing the trends, Redmonk analyst James Governor went as far as to declare "cloud [computing] is the new VMware." Naturally, then, the new VMware is cloud computing -- and that puts Java square in its sights.

[ Subscribe today to InfoWorld's Developer World newsletter and stay up to date on the key software development news and insights. ]

At the Google I/O conference in San Francisco this week, VMware and Google announced a partnership aimed at making it easier for Java developers to deploy applications on the Google App Engine cloud computing service. This new deal comes hot on the heels of a similar arrangement VMware struck with Salesforce.com in late April, in which VMware provided infrastructure to allow Java apps to run on Salesforce's Force.com platform.

With these partnerships, VMware is putting its customers on notice that it's not merely a virtualization vendor, but a full-service provider of solutions for cloud computing. But developers should take notice, too, because if VMware continues down this path, it could potentially mean a whole new era for the Java platform.

Write once, cloud everywhere

Sun Microsystems dubbed Java the "write once, run anywhere" language, but Java developers have always taken that phrase with a grain of salt. Early in the platform's history, Microsoft deliberately tried to fragment the Java market by shipping its own, incompatible version -- but even Sun's own JVM could be inconsistent from OS to OS. Apple, once a Java backer, dropped support for the language from its Mac OS X developer platform and won't allow it on the iPhone or iPad. The Java ME market is hopelessly fragmented. Browser applets are dead.

The bright spot for Java has always been the datacenter. But as I've mentioned before, cloud computing has the potential to fragment the server-side Java market, by encouraging developers to code their applications to a specific cloud vendor's services and requirements. For example, unlike Force.com,
================================================================================
Rank = 60; Score = 3899392.0
<|begin_of_text|>Anyone who had to schedule an intercontinental phone call knows that there is no such thing as a simple time called now. What you should rather think about is a time comprised of here and now.

The Earth rotates around its own axis. When it’s solar noon (the sun is at its highest position) in one place, it’s already past noon in places to the east and it’s still before noon in places to the west.

To make communication easier, at the end of the 19th century, the Earth was divided into 24 hour-wide time zones. All places within one time zone have the same time. It was decided that the “0” time zone would be London’s time zone. The “0” time zone is nowadays called UTC (Coordinated Universal Time) or, for historic reasons, GMT (Greenwich Mean Time). In general, the time zones’ boundaries follow countries’ boundaries though some large countries are located in multiple time zones. For example, USA is located in 7 different time zones, Canada in 6, but China is in one time zone. There are time zones with offsets of 30 minutes or even 45 minutes (Nepal). To make things even more complicated, many countries also have daylight saving time: they turn the clock back or forward one hour on specific days to account for seasonal changes in daylight.

If your computer system is international, it has to be ready to handle users from different time zones. The users want to see the time of system events expressed in their local time zone:

In an online course, you want to see the assignment deadline in your own time zone. You don’t want to submit your assignment and find out that the deadline has passed.

In an auction system you want to know at exactly what time the auction ends. You don’t want to try bidding and find out that the auction ended three hours earlier.

You want to know the time when a promotion ends. Ideally, if the promotion ends on a specific day, you want the promotion to run until the end of the day in UTC-11 time zone.

All databases have types which handle both date and time. The types are called timestamp or datetime, or something similar. They usually come in two flavors: one without time zone and the other with time zone.

Timestamp without time zone is generally a fancy string value. It does not interpret the time zone. If you enter the value “2015-01-28 13:00:00,” the same value will be shown to all users.


================================================================================
Rank = 61; Score = 3866624.0
<|begin_of_text|>Interpreted, dynamically typed languages such as Python, Ruby or Javascript are: easy to use, allow fast development and have a great number of libraries. Unfortunately, they are not very good when it comes to performance.

For this reason, I felt the need to look for something different for some projects. However, I didn't want to lose the simplicity other languages gave me. Essentially, I wanted something that could give me:

Good performance

Easy concurrency

Easy error handling

Safety

Fast development time

Good libraries to ease my development

Go was the obvious choice - I had previously worked with it and knew I could achieve all of these points.

So what is Go?

From the Go documentation page

Go is an attempt to combine the ease of programming of an interpreted, dynamically typed language, with the efficiency and safety of a statically typed, compiled language.

An attempt which I believe was mostly successful. But let's dive into some points to understand Go's beauty.

Simple and easy

Go is a "simple" language - it only contains 25 keywords (Python 3.5.0: 33, Ruby 2.2.0: 41, ANSI C: 32). Obviously, a small number of keywords doesn't necessarily mean it's easy - Brainfuck only has 8 and its name suggests how easy it is to program in it.

And Go does have trade-offs for using such a small number of keywords. For example, it only has one loop (keyword for ), which can be seen has a nice thing since you really only have to know one word. But it can take several forms:

for condition behaves like a while

behaves like a while for i,v := range list ranges over a list

ranges over a list for i=0;i<10;i++ for a C style classical for

which might confuse/annoy some people.

However, I do believe Go is easy to understand and learn, and you can see it for yourself by doing the

Golang Tour, which will teach you the essentials.

Standard Library

Furthermore, Go has got a robust standard library which allows quick and easy development of common essential tasks:

net (http, mail)

archives (zip, tar, gzip)

encodings (json, csv)

cryptography (aes, des, md5)

html template system

generic sql drivers

logging system

and many others...

Easy to read

Any programmer can tell you that reading other people's (including your past self) code is a daunting, hair-
================================================================================
Rank = 62; Score = 3833856.0
<|begin_of_text|>There are some reasons why anyone will prefer visiting the casinos in Malaysia than with most other places. Malaysian casinos have several advantages over most casinos available other places in the world. These advantages make them popular in so many ways over the rest of casinos in other places. Below is an outline of the reasons why the next time you are out gambling you should think about hitting the Malaysia casino.

Different Games

In Malaysia, you will get to enjoy some games from blackjacks, betting to poker among others. With so many games to choose from, it is hard to get bored playing. If one game does not appeal to you, you can move on to the next one and get to enjoy. Having a pool of games form which to choose from will always make the gambling interesting. It will also attract different kinds of people. Having different people to play against will in a great way increase the chances that you have of making a good return.

Great Payouts

Another thing that makes most people prefer playing in Malaysian casinos is because of the great payouts that are offered. From the games that are available, it is quite easy t even double your initial investment in just a few minutes of playing. You will get to enjoy great payouts that in most cases, you will win than lose. Having this in mind will always make gambling somewhat appealing. All that one needs to do is to exercise caution while playing.

Security

Malaysian casinos are some of the most secure casinos in the world. For a gambler, security is crucial since they deal with money. One needs to be sure that the money they win will be in safe hands. A casino that ensures that the player is protected will always be a preferred option by the players. Some of the casinos even go to extra lengths of providing escort for their players so that they do not get robbed.

Privacy

A casino player will always value their privacy. Malaysian casino knows and understands this. Great measures are usually taken to ensure that people who request to remain anonymous remain so. Some people come from high-end families or are powerful people in the country. They might be people who wouldn’t want their identities revealed. When this is the case, they would want to still enjoy their game without constantly worrying who might see them. Malaysian casinos will do all that is possible to ensure that the identity of such people is protected.

From the above, it is evident that Malaysian casinos are some of the best in the world. There is so much that they have put in to ensure that their clients have a wonderful experience at all
================================================================================
Rank = 63; Score = 3833856.0
<|begin_of_text|>Let be a natural number. A basic operation in the topology of oriented, connected, compact, -dimensional manifolds (hereby referred to simply as manifolds for short) is that of connected sum: given two manifolds, the connected sum is formed by removing a small ball from each manifold and then gluing the boundary together (in the orientation-preserving manner). This gives another oriented, connected, compact manifold, and the exact nature of the balls removed and their gluing is not relevant for topological purposes (any two such procedures give homeomorphic manifolds). It is easy to see that this operation is associative and commutative up to homeomorphism, thus and, where we use to denote the assertion that is homeomorphic to.

(It is important that the orientation is preserved; if, for instance,, and is a chiral 3-manifold which is chiral (thus, where is the orientation reversal of ), then the connect sum of with itself is also chiral (by the prime decomposition; in fact one does not even need the irreducibility hypothesis for this claim), but is not. A typical example of an irreducible chiral manifold is the complement of a trefoil knot. Thanks to Danny Calegari for this example.)

The -dimensional sphere is an identity (up to homeomorphism) of connect sum: for any. A basic result in the subject is that the sphere is itself irreducible:

Theorem 1 (Irreducibility of the sphere) If, then.

For (curves), this theorem is trivial because the only connected -manifolds are homeomorphic to circles. For (surfaces), the theorem is also easy by considering the genus of. For the result follows from the prime decomposition. But for higher, these ad hoc methods no longer work. Nevertheless, there is an elegant proof of Theorem 1, due to Mazur, and known as Mazur’s swindle. The reason for this name should become clear when one sees the proof, which I reproduce below.

Suppose. Now consider the infinite connected sum

This is an infinite connected sum of spheres, and can thus be viewed as a half-open cylinder, which is topologically equivalent to a sphere with a small ball removed; alternatively, one can contract the boundary at infinity to a point to recover the sphere. On the other hand, by using the associativity of connected sum (which will still work for the infinite connected sum, if one thinks about it carefully
================================================================================
Rank = 64; Score = 3817472.0
<|begin_of_text|>Subdomains can be a tricky thing to work with. Here are some tips to develop and test Rails applications using them.

Develop

During development there are a few ways you can handle subdomains. If you are using the bundled webrick server, you won't be able to access your subdomain at all.

Using lvh.me

If you are using the webrick server or something like Puma for development you can use lvh.me to access your subdomains. e.g.

http://sub.lvh.me:9292/

'lvh.me' resolves to localhost, so this provides access to the subdomain in Rails on port 9292 (the Puma default). This is convenient but can be very slow to work with.

Editing your hosts file

To avoid having to do a round trip to the internet you can add a record to your machine hosts file. In Unix/mac you can find it on /etc/hosts

You can add something like:

127.0.0.1 sub.virtual.local

Then you should be able to access your subdomain using the built in web server or puma by going to: (again 9292 is the port used by puma):

http://sub.virtual.local:9292/

Using Pow (Only available for Mac)

Pow makes it extremely easy to work with subdomains: - Install pow - Link your application - Access the application using a subdomain e.g. sub.app.dev

Have a look at the powder gem, it makes installing pow even easier.

Testing

Capybara needs to hit a real webserver in order to test your application. So you will have the same challenge as when accessing it on development.

CI

The easiest way is to use `lvh.me' for this (as long as your CI server can resolve it).

Add a couple of methods accessible by your tests, e.g. in spec/support/subdomains.rb :

def switch_to_subdomain ( subdomain ) # lvh.me always resolves to 127.0.0.1 Capybara. app_host = "http:// #{ subdomain }.lvh.me" end def switch_to_main_domain # Capybara.app_host = "http://lvh.me" end

Then in your feature test you can do:

switch_to_subdomain ('sub' )

Testing on your local machine

The above approach should work on your local machine as well, but it still wasteful as it is doing a round trip to lvh.me. I found it useful to do the
================================================================================
Rank = 65; Score = 3817472.0
<|begin_of_text|>Many software developers find they need to store hierarchical data, such as threaded comments, personnel org charts, or nested bill-of-materials. Sometimes it’s tricky to do this in SQL and still run efficient queries against the data. I’ll be presenting a webinar for Percona on February 28 at 9am PST. I’ll describe several solutions for storing and querying trees in an SQL database, including the design I call Closure Table.

In Closure Table, we store every path in a tree, not only direct parent-child references, but also grandparent-grandchild, and every other path, no matter how long. We even store paths of length zero, which means a node is its own parent. So if A is a parent of B, and B is a parent of C and C is a parent of D, we need to store the following paths: A-A, A-B, A-C, A-D, B-B, B-C, B-D, C-C, C-D, D-D. This makes it easy to query for all descendants of A, or all ancestors of D, or many other common queries that are difficult if you store hierarchies according to textbook solutions.

CREATE TABLE TreePaths (

ancestor CHAR(1) NOT NULL,

descendant CHAR(1) NOT NULL,

length INT NOT NULL DEFAULT 0,

PRIMARY KEY (ancestor, descendant)

) ENGINE=InnoDB;

Because there isn’t much written about using the Closure Table design, I periodically get questions about how to solve certain problems. Here’s one I got this week (paraphrased):

I’m using Closure Table, which I learned about from your book and your presentations. I can neither find nor invent a pure-SQL solution for moving a subtree to a new position in my tree. Right now, I’m reading the nodes of the subtree into my host script, deleting them and re-inserting them one by one, which is awful. Are you aware of a more efficient solution for moving a subtree in this design?

Moving subtrees can be tricky in both Closure Table and the Nested Sets model.Â It can be easier with the Adjacency List design, but in that design you don’t have a convenient way to query for all nodes of a tree. Everything has tradeoffs.

In Closure Table, remember that adding a node involves copying some of the existing paths, while changing the endpoint for descendant.Â When you’re inserting a single new node, you just have one descendant to add, joined to all the paths of its ancestors.

Here
================================================================================
Rank = 66; Score = 3801088.0
<|begin_of_text|>Every time someone tells me, “This database is mission critical – we can’t have data loss or downtime,” I just smile and shake my head. Technology is seriously difficult.

To illustrate, here’s a collection of client stories from the last few years:

The DBCC CHECKDB job ran every week just like it was supposed to – but it failed due to corruption every week. No one got email alerts because the SQL Agent mail was no longer valid – internal email server changes meant the mail was just piling up in SQL Server. CHECKDB had been failing for three years, longer than the backups were kept. Data was permanently lost. The DBA configured his backups to write to a file share. The sysadmins never understood they were supposed to back up that file share. When the DBA asked for a restore, he was surprised to find there were no backups. Three SQL Servers were all replicating data to each other. When I asked the DBA where the backups were run, he looked at one server, then another, then the third. He sheepishly admitted – in front of his manager – that there were no backups done anywhere. The DBA set up full backups daily, plus log backups of all databases in full recovery mode. Later, she put a few databases into simple recovery mode in order to fix an issue. She forgot to put them back into full recovery mode. When problems struck and she needed to recover a database, she lost all data back to the prior full backup. The SQL Server ran out of space on the C drive. During emergency troubleshooting, someone deleted a bunch of BAK files. The server started up, but databases were offline and corrupt. Turned out the user databases were on the C drive, as were all of the backups – the very backups that were just deleted to free up space. The DBA started getting odd corruption errors on one of his servers, then more, and quickly all of them. The SAN admin had flashed the storage with new firmware – which had a bug. The DBA was writing his backups to that same SAN, and sure enough, some of the corrupt databases had corrupt backups too. The admin wanted to restore the production databases onto another server. He tried, but it kept saying the files were in use. He stopped the SQL Server service, deleted the files, started it again, and finally his restore worked – but his phone lit up. Turned out he’d remote desktopped into the wrong server – he was on production. The developer did a deployment on
================================================================================
Rank = 67; Score = 3784704.0
<|begin_of_text|>To extend the functionality of WordPress most people have only heard of the use of plugins. Not many people have heard of the term Dropins. WordPress has it's core functionality which can be added to by the use of plugins which take advantage of multiple WordPress hooks and actions, but it also allows you to replace functionality with the use of Dropin files. Unlike a plugin the Dropin file will not need to be activated and will become activate when it is placed in the wp-content folder. This folder will by default be at the root of your WordPress install, but can be defined by changing the constant variable WP_CONTENT_DIR.

// Custom content directory define( 'WP_CONTENT_DIR', dirname( __FILE__ ). '/wp-content' ); define( 'WP_CONTENT_URL', 'http://'. $_SERVER['HTTP_HOST']. '/wp-content' );

An example of how WordPress will include these Dropin files can be seen in the WordPress core code, here is an example of displaying the maintenance.php as you can see it uses the constant WP_CONTENT_DIR.

if ( file_exists( WP_CONTENT_DIR. '/maintenance.php' ) ) { require_once( WP_CONTENT_DIR. '/maintenance.php' ); die(); }

When you place your Dropin files in the wp-content folder you can see this from the plugin maintenance screen /wp-admin/plugins.php?plugin_status=dropins. To get a list of available dropin's there is a function in /wp-admin/includes/plugin.php called _get_dropins(), this will return the following list of dropin's. ## Single Site Install

advanced-cache.php - Advanced caching plugin. Allows you to replace the caching functionality of your WordPress site. Activated by defining a constant variable WP_CACHE in the wp-config.php file.

db.php - Custom database class. Used to create you own database class. Activated on load.

db-error.php - Custom database error message. Used to display your own custom database error message. Activated on load.

install.php - Custom install script. Used to customise your own WordPress install script. Activated on load.

maintenance.php - Custom maintenance message. Used to create your own WordPress custom message. Activated on load.

object-cache.php - External object cache. Used to create your own object caching class. Activated on load.

Additional Multisite Dropin's<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 68; Score = 3784704.0
<|begin_of_text|>Most likely, as a reader you are mostly interested in my scripts. Here are my scripts for setting up Windows Server 2012 R2 ready for WebDeployments and running EPiServer ASP.NET site!

Why automate the installation?

This blog is about how to make your brand new Windows Server ready for WebDeployments with just pressing enter once. Cloud services can make your infrastructure lifecycle handling very easy; still people often encounter situations where they need to host our ASP.NET applications in virtual machines or directly on physical hardware. On those situations installation procedures in Windows operating systems are often done with some “clickety click” magic that won’t take too long. Still the “clickety click” installations have lots of long-term problems:

Installations can’t be reproduced

Only the installer knows how he did it

If you have multiple servers you are doing same manual steps multiple times

Base of the installations does not differ much from project to project

After few years when your windows server needs upgrade you will need to repeat this

There is no way to test “clickety click” installations

Windows Server Core installations are rare because people is not used to manage Windows Servers without GUI

One could argue that documentation and clear processes would take care of all the problems above. Maybe they could but I have never seen installation documentation that has 100% coverage over how the installation has been done. Installation script works as document and developers are more likely to update it!

What needs to be installed on a fresh Windows server

Here is short version of my list what I would do for new Windows Server

Install IIS

Install WebPI

Install newest.NET

Install webdeploy

Install various modules for IIS from windows feature list or with WebPI

Install tools for administration (7zip, text editor etc)

Create IIS site and application pool and change some defaults for better

Is there PackageManagement for Windows?

Oh yes there is! It is called Windows PackageManagement (aka OneGet). Unfortunately it is not available for Windows Servers yet. Production preview of Windows Management Framework 5 is already available. Windows PackageManagement is actually a manager that can access multiple type of repositories to have unified way to download and install software from multiple sources. Microsoft MSI installers, Windows Features and software could be all installed with Windows PackageManagement in the future. The thing still missing is grown up economy where all the toys would be easily available in a secure manner. Chocolatey is pretty good already but it is missing a lot of software and I’m still sceptical about
================================================================================
Rank = 69; Score = 3768320.0
<|begin_of_text|>On October 13, our teams will be performing an extended maintenance to make networking improvements on our servers. This maintenance will affect our Arc platform as well as all pages on our arcgames.com domain and games hosted from our servers that are launched from Arc and other platforms. The following games and services will be inaccessible during maintenance:

Arc Games forums

Arc Games main website, product pages and news pages

Arc Games platform including billing and messaging

Arc Games support websites

Battle of the Immortals

Blacklight Retribution (launched from Arc Games)

Champions Online

Forsaken World

Fortuna

Jade Dynasty

Neverwinter (PC & Xbox One)

PWI

Star Trek Online

Swordsman

War of the Immortals

Additional games launched from Arc Games

We anticipate this maintenance to run from October 13 starting at 6:00 am PT and running to 6:00 pm PT. We will be providing updates via our Arc Twitter page and Facebook page as well as social channels for each of our titles.

We appreciate your patience as we work to improve our platform.

Arc Team<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 70; Score = 3768320.0
<|begin_of_text|>Windows Azure: General Availability of Infrastructure as a Service (IaaS) Tuesday, April 16, 2013

This morning we announced the general availability release of our Infrastructure as a Service (IaaS) support for Windows Azure – including our new Virtual Machine and Virtual Network capabilities. This release is now live in production, backed by an enterprise SLA, supported by Microsoft Support, and is ready to use for production apps. If you don’t already have a Windows Azure account, you can sign-up for a free trial and start using it today.

In addition to supporting all of the features and capabilities included during the preview, today’s IaaS release also includes some great new enhancements:

New VM Image Templates (including SQL Server, BizTalk Server, and SharePoint images)

VM Image Templates (including SQL Server, BizTalk Server, and SharePoint images) New VM Sizes (including Larger Memory Machines)

VM Sizes (including Larger Memory Machines) New VM Prices (we’ve reduced prices 21%-33% for IaaS and PaaS VMs)

Below are more details on today’s release and some of the new enhancements. You can also read Bill Hilf’s blog post to learn about some of the customers who are already using the IaaS capabilities in production.

Windows Azure Virtual Machines

Windows Azure Virtual Machines enable you to deploy and run durable VMs in the cloud. You can easily create these VMs from an Image Gallery of pre-populated templates built-into the Windows Azure Management Portal, or alternatively upload and run your own custom-built VHD images. Our built-in image gallery of VM templates includes both Windows Server images (including Windows Server 2012, Windows Server 2008 R2, SQL Server, BizTalk Server and SharePoint Server) as well as Linux images (including Ubuntu, CentOS, and SUSE Linux distributions).

Windows Azure uses the same Hyper-V virtualization service built-into Windows Server 2012, which means that you can create and use a common set of VHDs across your on-premises and cloud environments. No conversion process is required as you move these VHDs into or out of Windows Azure – your VMs can be copied up and run as-is in the cloud, and the VMs you create in Windows Azure can also be downloaded and run as-is on your on-premise Windows 2012 Servers. This provides tremendous flexibility, and enables you to easily build hybrid solutions that span both cloud and on-premises environments.

Easy to Get Started

You can quickly create a new VM
================================================================================
Rank = 71; Score = 3751936.0
<|begin_of_text|>Press

Web Search By The People, For The People: YaCy 1.0

on: 2011-11-28

The YaCy project is releasing version 1.0 of its peer-to-peer Free Software search engine. The software takes a radically new approach to search. YaCy does not use a central server. Instead, its search results come from a network of currently over 600 independent peers. In such a distributed network, no single entity decides what gets listed, or in which order results appear.

The YaCy search engine runs on each user's own computer. Search terms are encrypted before they leave the user and the user's computer. Different from conventional search engines, YaCy is designed to protect users' privacy. A user's computer creates its individual search indexes and rankings, so that results better match what the user is looking for over time. YaCy also makes it easy to create a customised search portal with a few clicks.

"Most of what we do on the Internet involves search. It's the vital link between us and the information we're looking for. For such an essential function, we cannot rely on a few large companies, and compromise our privacy in the process," says Michael Christen, YaCy's project leader. "YaCy's free search is the vital link between free users and free information. YaCy hands control over search back to us, the users."

Each YaCy user is part of a large search network. YaCy is already in use on websites such as sciencenet.kit.edu, yacy.geocaching-portal.com, or fsfe.org, to provide a site-wide search function that respect users' privacy. It contains a peer-to-peer network protocol to exchange search indexes with other YaCy search engines.

"We are moving away from the idea that services need to be centrally controlled. Instead, we are realising how important it is to be independent, and to create infrastructure that doesn't have a single point of failure," says Karsten Gerloff, President of the Free Software Foundation Europe. "In the future world of distributed, peer-to-peer systems, Free Software search engines like YaCy are a vital building block."

Everyone can try out the search engine at http://search.yacy.net/. Users can become part of YaCy's network by installing the software on their own computers. YaCy is Free Software, so anyone can use, study, share and improve it. It is currently available for GNU/Linux, Windows and MacOS. The project is also
================================================================================
Rank = 72; Score = 3751936.0
<|begin_of_text|>Sharing Lua files in Renoise

Recent articles showed how to use to Renoise Lua scripting to alter a track’s send device. One was driven by OSC, the other by MIDI.

In both of them the code that did the actual send device manipulation was the same. But since these were two different scripts the same code appeared in both places.

Sharing code using copy-and-paste is not the world’s worst programming sin, however much it might make some developers cringe. If you are throwing together a one-off script for a short-lived need, go for it.

In many cases, though, having the same code in multiple places is a poor idea because it breaks one of the golden rules of software development: Be lazy. (Read up on the three great virtues of a programmer.)

Copy-and-paste sure seems like the laziest thing to do at first, but if the code has any amount of complexity or importance there’s a really good chance that sooner or later you will want to change it. Maybe to fix a bug, maybe to make it faster, maybe to add a feature.

With the same code in multiple places you need to keep track of all those places and edit the code in all those places. Oh, bother.

Can Renoise Lua code be shared? Can you create a file that can be loaded and used by multiple scripts?

Indeed you can.

Know your path, change your path

The preferred way to load files in Lua programs is the require function.

require looks at a special path that defines where files may be found. This path is actually many paths, and it is not just a set of folder paths but paths that contain some pattern-matching variables. (See that previous link for details.)

This path-of-paths can be altered. The usual warning about messing with program defaults applies here; if you screw up the require path your program may not run as expected (or at all) and you will be sad.

The path is a string, so here’s an example of changing it so that a tool file can load from a parallel directory:

package.path = "../ng-shared/?.lua;".. package.path

For example, suppose you have this folder and file layout:

Scripts/Tools/ng-shared/Utils.lua Scripts/Tools/com.neurogami.MidiMapper.xrns/main.lua

If the file main.lua in com.neurogami.MidiMapper.xrns has that path alteration then when it does this:

require "Utils"

the program will look in the ng-shared/ folder since it
================================================================================
Rank = 73; Score = 3735552.0
<|begin_of_text|>At its main plant in Ingolstadt, German carmaker Audi has for the first time deployed a robot that works ‘hand-in-hand’ with humans – without a safety barrier and ideally adapted to the employees’ working cycles.

It is the first human-robot cooperation at the Volkswagen Group to be applied in final assembly. This innovative technology makes work easier for the assembly employees and makes ergonomic improvements.

For Dr. Hubert Waltl, Board of Management Member for Production at Audi AG, human‑robot cooperation opens up entirely new possibilities: “The factory of the future will feature increasing interaction between man and machine. That allows us to automate routine operations and to optimise ergonomically unfavorable workplaces.” But also in the future, there will be no factory without people. “People will continue to make the decisions on production processes. And our employees will continue to be essential for future-oriented, successful production.”

Peter Mosch, chairman of the Group Works Council of Audi AG, commented: “We see the opportunities presented by the advancing interaction between man and machine. The decisive aspect for us is how this development is guided. We welcome it when it neither jeopardizes jobs nor leads to people losing independence to machines.”

For the employees of the A4/A5/Q5 assembly lines at Audi’s Ingolstadt plant, the new, direct cooperation between humans and robots is an enormous help: Until now, they have had to bend over material boxes to take out the coolant expansion tanks. At first glance, this seems like a simple task, but with frequent repetitions it can lead to back problems. From now on, the task will be taken over by a robot, known internally as ‘PART4you’.

It works hand-in-hand with the Audi employees and is fitted with a camera and an integrated suction cup. This enables it to pick up the components from the boxes and to pass them to the assembly workers – without any safety barrier, at the right time and in an ergonomically optimal position. “In a production process with increasing diversity of model versions, PART4you provides the employees with important assistance. It selects the correct component and holds it ready to be taken. This means that the employees no longer have to reach over long distances or bend down repeatedly. The robot becomes an assembly assistant operating at the same speed as the assembly worker – and not the other way around,” said Johann Hegel, Head of Assembly Technology Development.

“Thanks to a soft protective skin with integrated safety sensors, there is no danger to the employees,” explained Hegel. Because PART
================================================================================
Rank = 74; Score = 3735552.0
<|begin_of_text|>Good aim is essential to having fun and improving your results in CoD.

I have seen this question asked numerous times “How do I get a better aim in Call of Duty” and today we are going to go over exactly that. This will be very in-depth and we will go through every aspect in order for you to be the best you can be. However, there’s a common misconception that things will change overnight although that is not the case, these things take time and if you’re willing to put in the time then great things will happen to your gameplay and you will notice drastic improvements.

Vibration – Good Or Bad?

By default on Call of Duty, your controller is set to vibrate when you shoot and get shot. This adds a whole element to the game as it allows you to virtually feel everything your character does. However, because you get so used to it you are unable to play without it.

If you are using vibration, I would recommend turning it off immediately because the rumble from the vibrations will throw your shot off and make it harder to control your aim. It is only a minor change but it can make all the difference to your game and elevate it to that next level.

Addons – Kontrol Freeks

If you haven’t heard of Kontrol Freeks already, they are an extension that sits on top of your analogue sticks. You may now be asking yourself “Why would I want this?” Well, they are much taller than regular analogue sticks, it means you don’t need to move your thumbs as much whilst aiming. This will allow you to make much more precise movements, thus making it easier to play on higher sensitivities, although not many professional players do play at a higher sensitivity, however, we will get onto that topic shortly.

They are used by a lot of professional Call of Duty players.They are one of the most notorious companies when it comes to the gaming accessory industry and has a great reputation. Not only is the quality of the product exceptional, but also because they are considerably cheap. A pair of Kontrol Freeks will only cost you a mere $10-20 so they are definitely worth trying out. The best part is, if you don’t like them, they have a 30-day money back guarantee, so there is no risk to be had.

Sensitivity Levels – Very Controversial

There is no such thing as a ‘best sensitivity level’ at all. Some players play on 3, others play on 15, it’s all about what makes you
================================================================================
Rank = 75; Score = 3719168.0
<|begin_of_text|>Motivation

Last week we were discussing, among many other things, ways to speed up Firefox during startup. One obvious option was to move more of our I/O off of the main thread. This in turn involves making more code asynchronous, and asynchronous code is simply harder to manage. Mike Shaver mentioned something about "futures" as a possible way to handle the extra complexity, and then the discussion moved on to something else.

What exactly is a future anyway?

I'm not exactly an expert, but I've not just used futures, I've written my own implementations in JavaScript and even Object Pascal (in hindsight I'm not sure the latter was a good idea, but it was certainly an interesting exercise). Futures seem esoteric, but they really shouldn't be -- the idea is really quite simple. In this post I'll try to explain what futures are and how they can be used to make asynchronous programming easier.

In the simplest form, a future works like an IOU. I can't give you the money you've asked for right now, but I can give you this IOU. At some point in the future, you can give me the IOU and I'll give you the money -- if I have it. If I don't have it yet, then you can wait until I do. I get paid on Friday.

Alternatively there's the dry cleaner metaphor. You drop your clothes off on Monday and the clerk gives you a ticket that you can use later to reclaim your clothes after they've been cleaned. The clothes will be ready on Tuesday morning, but if you show up too early, you'll have to wait. On the other hand, if there's no hurry, you can just do other stuff on Tuesday and show up on Wednesday with a reasonable expectation that they'll be ready when you arrive. You'll just hand your ticket over, collect your clothes, and be on your way.

A future is similar to the IOU (or the dry cleaning ticket). It gives you a way to represent the result of a computation that has not yet completed, and it allows you to access that result once it becomes available. So you can call a function which starts some asynchronous process but doesn't wait for it to finish. Nevertheless the function can return you a useful result: a future which can be used to claim the real result later.

A simple example

Of course if you ask for the result too soon, you'll have to wait. On the other hand, if the result becomes available before you want it, then
================================================================================
Rank = 76; Score = 3702784.0
<|begin_of_text|>Sintel The Game is based on Blender Foundation's extremely popular movie 'Sintel'. The game will have action oriented gameplay while keeping same level of intensity and emotions that touched so many people. The story is as follows: After a long time, the developers of Sintel The Game posted about the game progress in a recent blog post.Sintel The Game is based on Blender Foundation's extremely popular movie 'Sintel'. The game will have action oriented gameplay while keeping same level of intensity and emotions that touched so many people. The story is as follows:

Sintel – The Game takes place within the events of the movie. Sintel finds herself just outside the town of Garway. She bumps into some bandits who attack her and she is taken into the care of a kind stranger. You play as Sintel and follow her on her journey through Garway. On the way you will discover that the guards of Garway are corrupt, and help to rise against them.

blog post: Excerpts from the

I worked on enemy AI a little bit, started re-designing the dialog system, and a bunch of other small things. Since I’ve gotten back, I started re-programming the player mechanics. I know this sounds silly seeing as we already have a working, controllable player, but there was room for improvement in the design and in the code. We also gave Sintel a nice new IK rig which needed to be implemented. As of right now, I’ve improved the movement system, making it easier to walk up and down hills, giving it a more realistic and less glitchy feel. It already feels a lot smoother than before. The code is optimized this time around too. Now i’m working on making the player camera better.

We still want to release a demo as soon as we can. We are slowly making our way towards a release. I think I speak for all of us when I say we don’t want to release a demo unless it is fun. The pre-demo was short and bland because we rushed it out without spending enough time on making it fun. The demo will include some different gameplay elements, a good taste of combat, and a boss battle that is actually challenging.

I can’t say when the demo will be ready, but the moment we pinpoint a date, I will let you guys know.

here. You will need Blender 2.5+ for the game to work. The game can be played by running sintel_the_game.blend file. To start the game press 'P' and
================================================================================
Rank = 77; Score = 3686400.0
<|begin_of_text|>Current trends in software and backend architecture have been evolving towards a more loosely coupled more granular design. I am sure most of you have heard of microservice based architectures. read more

The latest development on that front in the past couple of years has been the advent of serverless which allows you to run applications in very cost effective ephemeral services. This is why it is important to have a proper gateway for your API that is able to route all your requests to the designated endpoint.

GraphQL stands out in that respect as being a mature open sourced standard started at Facebook. We will first have a look at how we set up our own GraphQL server locally, then we will explore the Query language and schema definitions it provides which allows you essentially query your mesh of services from a single point of entry. The beauty of that is it will notify you early if any of your endpoints is misbehaving or the schemas are out of date by erroring out. Another advantage of this is that it allows for your API documentation to be a real time process and it will give you what one may call an API playground where you can query and explore your API.

After we explore our serverless API we will have a look at the more advanced features and standards around mutators and resolvers and then we will close by going all in, full serverless and deploy our graphql server to a function in the cloud.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 78; Score = 3686400.0
<|begin_of_text|>How to Build a Monitoring and Analytics System for Your IoT Embedded Devices

Jonathan Seroussi Blocked Unblock Follow Following Sep 3, 2017

This post outlines a cookbook we used to create a monitoring system for a connected embedded system. We used it to create one that can monitor all type of devices (even those that run Bare-Metal and RTOS), and we thought it’d make sense to help others build similar systems. This cookbook could easily be used to monitor device operational data as well as your actual application data. After deploying an IoT system, you need to make sure everything is working as it should and close a feedback loop from the field. We’re about to show you how to build one using open source tools almost for free. We also have a post that outlines the need for such a system (Check It Out).

Let’s focus on three things:

Device and gateway code that streams data to a centralized backend. Backend and database to process and store the data. Frontend to display the data.

Once these three building blocks are in place, acting upon the monitored data and connecting it to your operation processes is relatively easy easy.

1. Device and Gateway Logging tool

When thinking about a logging utility hosted on your end devices and gateways, the things that come up are:

It needs to have a small footprint.

Never crash my device.

Consume little power.

Not disruptive to the application code.

Your devices means your code. It’s challenging to generalize this topic as a lot depends on the communication channel used to connect to the internet. We open sourced a logging tool that logs events from your end devices and sends it to your gateway/backend. If you are using nRF52 with BLE GATT and a gateway we also provide you with a sample that works out of the box. Go to our GitHub to get started.

1. jumper-ulogger — is an under 500B memory footprint logging framework you integrate into your end device (with examples for porting to nRF52 device and CC3200).

2. jumper-logging-agent — is the service that runs on the gateway.

3. jumper-ble-logger — is a logging agent service example for a BLE GW.

If you have any issues or question send us a note to info@jumper.io.

2. Backend and Database

This is where you should pay attention more carefully because there’s some good stuff here :-) I recommend using the following toolsets to make this work:

A key-value database to store a current snapshot of a device’s state
================================================================================
Rank = 79; Score = 3637248.0
<|begin_of_text|>HTML5 Video at Netflix

Netflix Technology Blog Blocked Unblock Follow Following Apr 14, 2013

by Anthony Park and Mark Watson

Today, we’re excited to talk about proposed extensions to HTML5 video that enable playback of premium video content on the web.

We currently use Microsoft Silverlight to deliver streaming video to web browsers on the PC and Mac. It provides a high-quality streaming experience and lets us easily experiment with improvements to our adaptive streaming algorithms. But since Microsoft announced the end of life of Silverlight 5 in 2021, we need to find a replacement some time within the next 8 years. We’d like to share some progress we’ve made towards our goal of moving to HTML5 video.

Silverlight and Browser Plugins

Silverlight is a browser plugin which allows our customers to simply click “Play” on the Netflix website and watch their favorite movies or TV shows, but browser plugins have a few disadvantages. First, customers need to install the browser plugin on their computer prior to streaming video. For some customers, Netflix might be the only service they use which requires the Silverlight browser plugin. Second, some view browser plugins as a security and privacy risk and choose not to install them or use tools to disable them. Third, not all browsers support plugins (eg: Safari on iOS, Internet Explorer in Metro mode on Windows 8), so the ability to use them across a wide range of devices and browsers is becoming increasingly limited. We’re interested to solve these problems as we move to our next generation of video playback on the web.

HTML5 Premium Video Extensions

Over the last year, we’ve been collaborating with other industry leaders on three W3C initiatives which are positioned to solve this problem of playing premium video content directly in the browser without the need for browser plugins such as Silverlight. We call these, collectively, the “HTML5 Premium Video Extensions”:

Media Source Extensions (MSE)

The W3C Media Source Extensions specification “extends HTMLMediaElement to allow JavaScript to generate media streams for playback.” This makes it possible for Netflix to download audio and video content from our content delivery networks and feed it into the video tag for playback. Since we can control how to download the audio/video content in our JavaScript code, we can choose the best HTTP server to use for content delivery based on real-time information, and we can implement critical behavior like failing over to alternate servers in the event of an interruption in content delivery. In addition, this allows us to implement our industry-leading adaptive streaming algorithms (real-time
================================================================================
Rank = 80; Score = 3538944.0
<|begin_of_text|>Word in China is out about blockchain technology, as the government made clear in an Informatization Strategy published in December of 2016. The strategy states, "The internet, cloud computing, large data, artificial intelligence, machine learning, blockchain … will drive the evolution of everything - digital, network and intelligent services will be everywhere."

It was an official endorsement for the new digital age and a big boost for blockchain technology.

In a country with $5.5 trillion in digital payments last year (50 times the U.S.), blockchain is now a buzzword among the titans of industry. And in the race to participate, Chinese banks, builders, suppliers and retailers are pumping out blockchain solutions.

Survival of the Fittest

Even with millions of dollars, many of these new blockchains may not make it very far. In a recent WeChat post, Antshares executive Erik Zhang stated that 90 percent of the enterprise blockchains we are seeing are doomed to fail. Without an open-source code that anyone can build upon, Zhang argues, private solutions will not see the network effect of a healthy ledger.

The big question for China, however, is whether a country known for its centralized authority, and a penchant for all things made-in-China, will allow an open-source, global standard solution sit on its internet. With Bitcoin, Ethereum and Hyperledger vying for dominance, there are big names within China making moves into the space.

A few of these big names include:

The People's Bank of China (PBOC) - The PBOC is reportedly close to the release of a government-backed digital RMB currency, which would put China at the frontier of digital currency adoption. And there are whispers within China that Shenzhen will be ground zero for the new digital economy. In September, Bloomberg reported that PBOC Vice Governor Fan Yifei wrote: "[T]he conditions are ripe for digital currencies, which can reduce operating costs, increase efficiency and enable a wide range of new applications." This would pave the way for blockchain startups in China to move forward in digital banking, finance, record-keeping, supply chains, IoT, AI and more.

Wanxiang Blockchain Labs - Working with Ethereum, Wanxiang is the largest blockchain development backer in China. After purchasing 500,000 ETH tokens last year, they pledged $30 billion for the development of a smart city in Hangzhou. They offer open-source platforms for anyone to build upon, and launched an accelerator fund for developers, intending to put money into
================================================================================
Rank = 81; Score = 3522560.0
<|begin_of_text|>Javascript React Get Started with redux-form

The redux-form library bills itself as the best way to manage your form state in Redux. It provides a higher-order form component and a collection of container components for dealing with forms in a React and Redux powered application. Most importantly, it makes it easy to get a form up and running with state management and validations baked in.

To get a feel for what redux-form can do for us, let's build a simple Sign In form. You can follow along below or check out the resulting source code directly.

Start with React

We are going to start by getting the React portion of this application setup. The easiest way to spin up a React app is with create-react-app. Install it with npm or yarn if you don't already have it globally available.

$ yarn global add create-react-app

Let's generate our project.

$ create-react-app redux-form-sign-in

The create-react-app binary that is now available on our machine can be used to bootstrap a React app with all kinds of goodies -- live code reloading in development and production bundle building -- ready to go.

Let's jump into our project and kick off a live reloading development server.

$ cd redux-form-sign-in $ yarn start

At this point you should see your browser pointed to localhost:3000 with a page reading Welcome to React.

Is This Thing Plugged In?

We can see the live-reload in action by altering src/App.js.

return ( < div className = "App" > < div className = "App-header" > < img src = { logo } className = "App-logo" alt = "logo" /> < h2 > Redux Form Sign In App < /h2 > < /div > < p className = "App-intro" > To get started, edit < code > src / App. js < /code> and save to reload. < /p > < /div > );

Change the text in the h2, save the file, and then switch back to the browser to see the changes almost instantly.

create-react-app made it really easy to get to a point where we can just iterate on our app. We didn't have to fiddle with Webpack configurations or any other project setup.

Next, let's change the prompt in the app intro.

< p className = "App-intro" > Sign in here if you already have an account < /p >

Our app is now begging for a form which brings us to redux-form, the focus of this post.

Satisfying
================================================================================
Rank = 82; Score = 3506176.0
<|begin_of_text|>The first final version of Hibernate OGM is out and we’ve recovered a bit from the release frenzy. So we thought it’d be a good idea to begin the new year with a tutorial-style blog series, which shows how to get started with Hibernate OGM and what it can do for you. Just in case you missed the news, Hibernate OGM is the newest project under the Hibernate umbrella and allows you to persist entity models in different NoSQL stores via the well-known JPA.

We’ll cover these topics in the following weeks:

Persisting your first entities (this instalment)

Querying for your data

Running on WildFly

Running with CDI on Java SE

Store data into two different stores in the same application

If you’d like us to discuss any other topics, please let us know. Just add a comment below or tweet your suggestions to us.

In this first part of the series we are going to set up a Java project with the required dependencies, create some simple entities and write/read them to and from the store. We’ll start with the Neo4j graph database and then we’ll switch to the MongoDB document store with only a small configuration change.

Let’s first create a new Java project with the required dependencies. We’re going to use Maven as a build tool in the following, but of course Gradle or others would work equally well.

Add this to the dependencyManagement block of your pom.xml:

... <dependencyManagement> <dependencies>... <dependency> <groupId> org.hibernate.ogm </groupId> <artifactId> hibernate-ogm-bom </artifactId> <type> pom </type> <version> 4.1.1.Final </version> <scope> import </scope> </dependency>... </dependencies> </dependencyManagement>...

This will make sure that you are using matching versions of the Hibernate OGM modules and their dependencies. Then add the following to the dependencies block:

... <dependencies>... <dependency> <groupId> org.hibernate.ogm </groupId> <artifactId> hibernate-ogm-neo4j </artifactId> </dependency> <dependency> <groupId> org.jboss.jbossts </groupId> <artifactId> jbossjta </artifactId> </dependency>... </dependencies>...

The dependencies are:

The Hibernate OGM module for working with an embedded Neo4j database; This will pull in all other required modules such as Hibernate OGM core and the Neo4j driver. When using
================================================================================
Rank = 83; Score = 3506176.0
<|begin_of_text|>CUSTOMIZER CHALLENGE CONTEST WINNER - Artistic Category This is a 100% printed customizable music box! Only 3D printed parts are used in the design and it can be assembled and disassembled via printed snapping mechanisms. The project originated when a friend of mine said that he'd only be interested in 3D printing once he can print a music box ;) Videos http://youtu.be/LUlovenI9xQ http://youtu.be/K_c3p24RRtQ (made by banthafodder7400) http://youtu.be/exNeQDz7f3g I'll try to keep the.scad file on this page updated but to help me to manage the design and to make it easier for others to contribute: https://github.com/wizard23/ParametrizedMusicBox

Updates

2018-03-28 Updated URL to generate parameters (was offline)

2016-09-20 Spreadsheet to help you to get the parameters right: https://goo.gl/sKWWDk (created by: Ben Horner)

2013-11-24 Updated link to generator page (old webserver is offline)

2013-03-10 (V3) Added optional Name of Song on top/bottom of MusicCylinder; fixed build plate positioning of pulley that messed up smaller customized versions; cleanup and clarification of descriptions; implemented work around for customizer hickup when strings start with a '.'

2013-03-08 (V2) removed "work in progress status", fully test printed

It's very important to put the music box on a sounding box to get good sound quality. I found that large cardboard boxes and some tables make good sounding boxes. A guitar or a piano should work even better!

A complete music box consists of 6 parts:

Case: the large thin walled part that holds the vibrating teeth and holds everything together

Music Cylinder: the large cylinder with the pins (that encode the music) sticking out

Transmission gear: sits between the crank gear and the music cylinder

Crank gear: drives everything, connects to crank (insert it through the round hole in the case)

Crank: for manually driving the box, connects to crank gear and crank pulley

Pulley: for holding the crank while turning it

With the default parameters you get a complete building plate that can play one full octave range (13 half notes from C to C) in a medium footprint that should fit in most printers. You can
================================================================================
Rank = 84; Score = 3506176.0
<|begin_of_text|>Microsoft is integrating Skype directly into Windows 10, and the result looks a lot like Apple's iMessage service. While the company unveiled some of its Skype integration at a special Windows 10 press event in Redmond yesterday, the software maker didn’t show its new Messaging app on the PC version of Windows 10. This appears to be key to a new experience for Skype messaging in Windows 10, and it brings back the built-in Messaging app from Windows 8 that Microsoft killed with the Windows 8.1 update.

Skype is starting to link usernames to mobile numbers

The new Messaging app works by integrating Skype, allowing you to chat to Skype contacts or initiate video / audio calls. All the conversations are synced between PCs, tablets, and phones, and the new app looks like a lightweight version of Skype. It’s also identical to the Messages app on OS X, with the same two-panel interface and circular UI for contact photos. Microsoft has started linking Skype usernames with mobile numbers to make it easier to find friends who are using the service without having to know their user ID. That makes this whole approach a lot more like iMessage, allowing Skype users to chat to friends easily on the service. The main difference is that Skype is cross-platform so you can chat to friends on iOS, Android, BlackBerry, Windows, and more, while iMessage is limited to Apple’s platforms.

The built-in Skype experience on the phone version of Windows 10 also allows you to send text messages, but it’s entirely possible (and likely) that Microsoft will extend this functionality and sync state to the PC version just like iMessage. In Microsoft’s new world Windows 10 apps are the same across PCs, phones, and tablets, so such a move would be expected. Microsoft isn’t fully detailing its Messaging app plans just yet, but it’s encouraging to see the company move to a more native and simple integration of Skype instead of separate and unnecessary apps. All that's needed now is the complexity of usernames to fully disappear so everyone can use Skype just with their mobile number.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 85; Score = 3489792.0
<|begin_of_text|>With the U.K. report on Facebook, and the stern language within it, the train on regulating data sharing may finally reach the station this year. The FTC is also likely to impose a stiff fine on Facebook for violating a consent decree.

So let's learn more about this data sharing business. If you prefer a video, the gist of this post can be heard here.

***

First, let's talk about data flows and the "cloud". Data are stored in computers that are called servers. In the cloud computing model, these servers are owned - not by the companies that collect the data - but by large tech companies like Amazon, Google, Microsoft, etc. who are responsible for managing the servers. These servers are geographically dispersed and so when data enter the cloud, they get replicated and spread to many servers. The technical benefit of such replication is recoverability of the data (allowing the use of cheaper, less reliable computers) but now, the data become much harder to delete.

Data become more telling if one combines different datasets measuring different aspects of our lives. For example, an auto insurer may have data on past claims and that data help predict your future claims. But if the auto insurer is able to get data from say an automaker about your car, e.g. how fast you drive, where you drive, etc., that data combined with past claims improve the predictive power.

Thus, a data-sharing industry has been created. Companies make agreements to share data with one another. This becomes much easier in the "cloud" as those servers are already connected to one another. These agreements may include explicit payments but even if they don't, both sides must be benefiting commercially from the arrangement, or else they would not exist.

So when company A shares data with company B, the data flow from A servers to B servers. B may also use a cloud, which then means the data would be replicated yet again, and dispersed geographically onto yet another set of servers.

And company B may also share data with company C, etc., etc.

***

An inexplicable part of the consent decree between Facebook and the FTC is the requirement that Facebook monitor what happened to the data after they are shared with third parties. I just can't figure out how that is possible. It isn't even possible within Facebook: if a user demands that his/her be deleted, it will be very hard to ensure that all copies of the data are deleted from every server, including data that might have landed in an analyst's computer. In fact, most analysts
================================================================================
Rank = 86; Score = 3489792.0
<|begin_of_text|>Google is launching a new enterprise service for large businesses that want to adopt Chrome OS devices. The new Chrome Enterprise subscription, which will cost $50 per device and year, is essentially a rebrand of Chrome Device Management, but with a number of additional capabilities. Even though the name would make you think this is about the Chrome browser, this program is actually all about Chrome OS. For Chrome users in the enterprise, Google already offers the Chrome Enterprise Bundle for IT, after all.

For enterprises, the main highlight here is that Chrome Enterprise is fully compatible with their existing on-premise Microsoft Active Directory infrastructure. Google senior director of product management for Android and Chrome for Business and Education Rajen Sheth told me that this has long been a stumbling block for many enterprises that were looking at adopting Chrome OS devices. With this update, enterprise users will be able to use their existing credentials to log into their Chrome OS devices and access their Google Cloud services — and IT admins will be able to manage their access to these devices and services.

It’s worth noting that Chrome OS admins could already enable other services that use the SAML standard to enable single sign-on for Chrome devices.

In addition, businesses also will now be able to manage their Chrome OS devices from the same enterprise mobility management solutions they already use, starting with VMware’s AirWatch. Support for similar services will launch in the future.

With this new licence, IT admins also will be able to set up a managed enterprise app store for their users. This feature is currently in beta and focuses on Chrome OS’s ability to run Android apps, which is currently available on many of the most popular Chrome devices in the enterprise.

Other benefits of the Chrome Enterprise subscription include 24/7 enterprise support, managed OS updates and printer management (you may laugh about this last one, but that’s something that still matters in many offices).

It’s no secret that Google is working hard to get more enterprises to adopt its various cloud-based services. Chromebooks have already found a lucrative niche in verticals like retail and education. To expand its market share, though, features like this new integration with AirWatch were sorely needed.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 87; Score = 3473408.0
<|begin_of_text|>This effectively means that anyplace you can type text becomes a potential storefront.

Before customers have even closed your book, they can already have ordered the next book in the series.

I've been wondering lately about whether bitcoin could make Amazon unnecessary. Here are my thoughts on the subject.If you've never heard of Bitcoin, it's basically the e-mail of money. You get yourself a bitcoin wallet program, which can either be an online program hosted by a company (like the bitcoin equivalent of gmail) or you download a wallet program to your home computer (like Outlook Express). Whichever method you choose, you get to create a "bitcoin address" for yourself which works like an e-mail address, except that you receive money instead of messages. A bitcoin address looks like this: 19v5V5P9infaQyCaRxWWeWoHnRbkX684PS. (This is for a Pet Rescue site.) So if you wanted to send money to this bitcoin address, you would basically just pop open your money-email program and hit send. The money appears in the recipient's "money inbox" in a few minutes to a few hours, depending on the sum you send. As for transaction fees, they are so minimal as to be nearly nonexistent--we are talking on the order of pennies. This makes it practically to send $0.99 to someone and not have the money eaten up in transaction fees.The really neat thing about bitcoin addresses, though, is that they are just a line of text. You could post one anywhere--in your forum signature, at the end of your book, in your mailing list, your tweet, or on your author webpage. Anywhere you can put an e-mail address, you can put a bitcoin address.Now let's ask ourselves: why do we sell our books on Amazon, and not on Goodreads or Twitter or Wattpad or our website any other site that offers free exposure for your book? Well, duh. Because Amazon has a big, fat "Buy" button and those other sites don't. But with bitcoin these other sites become storefronts--anywhere you can type your e-mail, you can type your money e-mail. This means that readers can buy direct from Goodreads or Wattpad--no need to go to Amazon at all. What's more, bitcoin enables "one click" purchases everywhere. You don't have to go through the hassle of registering up for an account or worrying about giving your credit card information to some stranger--
================================================================================
Rank = 88; Score = 3473408.0
<|begin_of_text|>The title is specifically about CloudFront and Laravel, but for the most part, this will apply to most web applications behind a reverse-proxy of some sort, be it a CDN, load balancer, or some other proxy type.

What we'll cover here:

What changes when using a reverse-proxy on your application What your application needs to do to get around these issues How to use the TrustedProxy Laravel package to take care of the details for you in a Laravel Application

Applications behind a proxy

For the launch of https://course.shippingdocker.com, I put CloudFront in front of a Laravel application running inside of a Docker container.

I wanted CloudFront for a few reasons:

Primarily, I wanted to make use of AWS's free, auto-renewed SSL certificate. This frees me from paying for an SSL certificate and/or managing something like LetsEncrypt. Secondly, I was planning on using a t2.nano instance on AWS and wanted to save it some CPU cycles from serving static assets. These instances are tiny, and start eating away at its small number of given CPU credits when it hits just 2% of CPU usage. (Having the site be as speedy as possible for those overseas was also part of this decision).

So, imagine this setup - HTTPS connections are made to course.shippingdocker.com. CloudFront receives this HTTPS request, and then forwards it to my site runnning on an EC2 instance (the "origin server"). The EC2 instance is listening on port 80 for HTTP connections.

Since CloudFront won't forward to an IP address, we need to give it a hostname to use. We can use the EC2 instance's public DNS name - I used http://ec2-34-197-131-119.compute-1.amazonaws.com in the case of my server (try it out, that URL will work).

What your application sees

So, CloudFront is receiving an HTTPS request and decrypting it (terminating the SSL connection). It then sends the decrypted HTTP request to my server (the origin server) using the network address ec2-34-197-131-119.compute-1.amazonaws.com.

Laravel only sees that 2nd request, and thus assumes:

It's receiving requests over http:// instead of https://, and thus will generate URI's with the the http:// scheme, including form submit URLs and redirect locations The domain name used and thus seen by the application is ec2-34-197-131-119.compute-1.amazonaws.com. Laravel will generate
================================================================================
Rank = 89; Score = 3473408.0
<|begin_of_text|>Congressional Democrats are proposing to ban plastic guns, following reports of major security lapses at the nation’s airports

Plastic guns can be even more dangerous than traditional firearms because they're harder to detect, says Rep. Steve Israel (D-N.Y.).

ADVERTISEMENT

The Undetectable Firearms Modernization Act, backed by Israel and several other Democrats, would prohibit the manufacture of entirely plastic guns. The legislation would require a major component of every gun to contain enough traces of metal to be detected.Israel plans to unveil the legislation Tuesday during a press conference at LaGuardia Airport in New York City, where he will draw a connection between his bill and recent high-profile airport security lapses.

"If detectable weapons can make it through security checkpoints, how can we expect to catch wrongdoers carrying undetectable plastic firearms?" Israel told The Hill. “What could be worse than a gun that can be used on an airplane, but cannot be detected on the security line because it’s plastic?”

"It’s time to modernize our airport security so the American people can count on it," he added.

The Transportation Security Administration (TSA) failed a recent sting operation in which undercover agents sneaked fake explosives and weapons through airport security in 67 out of 70 tests, or about 95 percent of the time.

Many of America’s busiest airports failed the tests.

In one test, a federal agent was able to sneak a fake bomb that was strapped to his back through security, even after he was subjected to a pat-down.

The massive security failure led to the ouster last week of TSA acting administrator Melvin Carraway.

In light of the TSA security lapses, Israel and other Democrats say it is even more important to ban plastic guns.

“The fact is, we should be making it easier, not harder for TSA to stop dangerous weapons from getting onto our planes,” Israel said.

“What angers me, and frankly what frightens me, is that the guns that were getting passed the TSA agents were highly detectable,” he added. “But a plastic gun couldn’t be picked up by the most astute and trained TSA agent.”

Plastic guns have been made popular by 3D printers that make it easier for consumers to build their own firearms.

While the legislation would still allow manufacturers to build partially plastic guns, it would close a loophole that allows people to build guns that can evade security.

Current law prohibits plastic guns, but gun owners can get around the requirements by including a detachable strip of metal on an otherwise plastic gun
================================================================================
Rank = 90; Score = 3440640.0
<|begin_of_text|>Microsoft just announced a service that will make it easier to manage storage for SharePoint Online within the Office 365 suite.

The move effectively tackles one of the big problems users were having with storage allocation to SharePoint site collections. It also makes the management of entire SharePoint environments easier.

The changes announced last night makes the storage limits associated with each group of sites more flexible. It means storage that is not used with one site collection can be moved to another to facilitate the development of that collection.

Freeing-Up Storage

Site collections are groups of related SharePoint websites that, until now, were assigned a set amount of storage space that could not be used for anything else even if the site collection did not use all that space.

Mark Kashman, senior product manager on the SharePoint marketing team, with Microsoft said in a blog post that from here on in, if an administrator sets a storage quota of 100 GB for a site collection and only uses 20 GB, under the old system the 80 GB remaining was wasted.

Under the new system, the 80 GB is returned to the storage pool meaning that site collections no longer "reserve" storage, and that they only use the storage they actually need.

As Needed

The process of assigning storage can also be automated with a tab that enables either manual, or automatic, of storage. The automatic setting increases storage requirements for site collections as needed, drawing on what's available from the pooled storage for the entire Office 365 domain.

The final change that Microsoft has introduced is that the number of site collections SharePoint Online supports per tenant has been increased from 10,000 to 500,000 with the total storage now allowed starting at 10 GB+ 500 MB for every user. This is separate from the default per-uses OneDrive for Business storage space.

The new storage limits will be available for most of the Office 365 plans including Office 365 Enterprise E1, E3, and E4; Office 365 Education A2, A3, and A4; Office 365 Government G1, G3, and G4. Office 365 Midsize Business will have this new storage model.

Guessing Game

The changes will be a welcome bonus to SharePoint administrators and site developers that have, until now, had to play a guessing game around the resources they will have to apply to site development.

Matter Center for Office 365

The announcement dovetails with the announcement that Microsoft has developed a document management add-on for Office 365 for the legal profession. The new app, called
================================================================================
Rank = 91; Score = 3440640.0
<|begin_of_text|>CoreOS founder Alex Polvi Twitter/@polvi

Google Ventures is leading a $12 million investment in CoreOS, a tiny startup that's changing the way modern web applications are built and maintained.

The startup will also use a recently released Google technology called Kubernetes in one of its own products, which is aimed at helping companies run their data centers more efficiently.

CoreOS makes a super-lightweight version of the free Linux operating system, which reduces the amount of hardware you need to run applications in big data centers. That's made it popular among software developers looking to do more with less.

For a while there, CoreOS was tight with Docker, one of the hottest startups in Silicon Valley, and it's no surprise why — they both had the mission of taking apps and putting them into what we call "containers."

Think of containers as a metaphor. Shipping yards put goods into a bunch of shipping containers all the same shape and size because it makes them easy to stack onto boats.

In the same way, containers let you take an application (an email server, a blogging platform, whatever) and whatever else it needs to run and throw it into a standard virtual "box." You can put this box on your own servers in your own data center, or upload it to Amazon's or Microsoft's or Google's cloud service and it'll run the exact same way, without the extra effort of having to struggle with it to make it work. It's all self-contained. (Hence, "containers.")

CoreOS made a version of Linux that went into these boxes, and Docker made the boxes themselves.

Docker Founder and CTO Solomon Hykes Flickr/ 97226415@N08

But Docker and CoreOS had a falling out late last year. A container on its own doesn't really do much, it needs to be overseen and managed. Docker wanted to sell this management technology as well.

CoreOS had an issue with what it saw as Docker getting too big for its britches, and made its displeasure clear with the public launch of Rocket, a competitor to Docker. Suddenly, developers had two options for building those self-contained apps-in-boxes.

Meanwhile, Google has quietly been using containers in its own huge data centers for many years now. As Docker made containers popular to the rest of the world, Google released some of its own technology for managing them, dubbed Kubernetes, for free. Kubernetes seen some uptake in both the CoreOS Rocket and Docker communities.

Today, in addition to the funding, CoreOS is announcing
================================================================================
Rank = 92; Score = 3440640.0
<|begin_of_text|>It’s one thing to know about SQL Injection, File Access Attacks, XSS, and other security hazards. And because you’re a great developer you’re regularly squashing vulnerabilities in your app. And yet every Ruby/Rails developer is relying on someone else’s code to do their work. And guess what? No matter how careful you are, no matter how much time you spend perfecting your code, someone else’s code is going to have a security bug (yours will too if you & I are being honest with each other!)

One of the questions that a few people have emailed me with now is: “How do I stay up to date on Ruby/Rails security?” It’s a great question! First because not enough developers care about security. Second because there are a lot of great tools out there to help protect your app. Let’s look at how you can stay up to date.

By the way feel free to email me@gavinmiller.io if you have questions too!

Follow Relevant Mailing Lists

The first step in keeping your app up to date and protected is to keep up with the news. The two main sources of security news are the Ruby Security Mailing List and the Rails Security Mailing List. Both lists focus on security and will give you the best warning that an attack/fix is coming down the pipe.

Follow CVE Reports

Now the Ruby, and Rails mailing lists are great, but you have A LOT more dependencies in your app than that: nokogiri, rack, thin, puma, etc.. Unless there was a major issue in these gems they’re not going to make the Rails or Ruby mailing list, so you need to get that information from elsewhere!

One of the little know resources for keeping up with security vulnerabilities are CVE databases. There are a few different sites that offer this type of information and CVE Details is my favorite because it’s easy to consume the information.

Ruby and Rails both have dedicated pages, and you can create an RSS feed of those pages. And for the major gems in your site navigate to their pages and create an RSS feed for them as well!

Keep Code Updated

A simple way to keep your application up to date with the latest vulnerabilities is to not let your dependencies become outdated. To do this run bundle outdated on your codebase and update the gems that are out of date.

This is usually easier said than done because updating dependencies can cause your application to break in unexpected ways. The mitigation for that is keeping your tests up to date. If you can update
================================================================================
Rank = 93; Score = 3424256.0
<|begin_of_text|>AWS to Azure services comparison

In this article

This article helps you understand how Microsoft Azure services compare to Amazon Web Services (AWS). Whether you are planning a multicloud solution with Azure and AWS, or migrating to Azure, you can compare the IT capabilities of Azure and AWS services in all categories.

In the following tables, there are multiple Azure services listed for some AWS services. The Azure services are similar to one another, but depth and breadth of capabilities vary.

Azure and AWS for multicloud solutions

As the leading public cloud platforms, Azure and AWS each offer businesses a broad and deep set of capabilities with global coverage. Yet many organizations choose to use both platforms together for greater choice and flexibility, as well as to spread their risk and dependencies with a multicloud approach. Consulting companies and software vendors might also build on and use both Azure and AWS, as these platforms represent most of the cloud market demand.

For an overview of Azure for AWS users, see Introduction to Azure for AWS professionals.

Marketplace

Area AWS service Azure service Description Marketplace AWS Marketplace Azure Marketplace Easy-to-deploy and automatically configured third-party applications, including single virtual machine or multiple virtual machine solutions.

Compute

Storage

Networking and content delivery

Area AWS service Azure service Description Cloud virtual networking Virtual Private Cloud (VPC) Virtual Network Provides an isolated, private environment in the cloud. Users have control over their virtual networking environment, including selection of their own IP address range, creation of subnets, and configuration of route tables and network gateways. Cross-premises connectivity AWS VPN Gateway Azure VPN Gateway Azure VPN Gateways connect Azure virtual networks to other Azure virtual networks, or customer on-premises networks (Site To Site). It also allows end users to connect to Azure services through VPN tunneling (Point To Site). Domain name system management Route 53 Azure DNS Manage your DNS records using the same credentials and billing and support contract as your other Azure services Route 53 Traffic Manager A service that hosts domain names, plus routes users to Internet applications, connects user requests to datacenters, manages traffic to apps, and improves app availability with automatic failover. Content delivery network CloudFront Azure Content Delivery Network A global content delivery network that delivers audio, video, applications, images, and other files. Dedicated network Direct Connect ExpressRoute Establishes a dedicated, private network connection from a location to the cloud provider (not over the Internet). Load balancing Classic Load Balancer

Network Load Balancer

Application Load Balancer Load Balancer

Application Gateway Automatically distributes incoming application traffic to add
================================================================================
Rank = 94; Score = 3407872.0
<|begin_of_text|>How to speed up your MySQL with replication to in-memory database

Vadim Popov Blocked Unblock Follow Following Mar 17, 2017

Original article available at https://habrahabr.ru/company/mailru/blog/323870/

I’d like to share with you an article based on my talk at Tarantool Meetup (the video is in Russian, though). It’s a short story of why Mamba, one of the biggest dating websites in the world and the largest one in Russia, started using Tarantool. Why did we decide to busy ourselves with MySQL-to-Tarantool replication?

First, we had to migrate to MySQL 5.7 at some point, but this version didn’t have HandlerSocket that was being actively used on our MySQL 5.6 servers. We even contacted the Percona team — and they confirmed MySQL 5.6 is the last version to have HandlerSocket.

Second, we gave Tarantool a try and were pleased with its performance. We compared it against Memcached as a key-value store and saw the speed double from 0.6 ms to 0.3 ms on the same hardware. In relative terms, Tarantool’s twice as fast as Memcached. In absolute terms, it’s not that cool, but still impressive.

Third, we wanted to keep the whole existing architecture. There’s a MySQL master server and its slaves — we didn’t want to change anything in this structure. Can MySQL 5.6 slaves with HandlerSocket be replaced with something else without having to make significant architectural changes?

We learned that the Mail.Ru Group team has a replicator they created for their own purposes. The idea of replicating data from MySQL to Tarantool belongs to them. We asked the team to share the source code, which they did. We had to rewrite the code, though, since it worked with MySQL 5.1 and Tarantool 1.5, not 1.7. The replicator uses libslave, an open-source solution for reading events from a MySQL master server, and is built statically without any of MySQL’s system libraries. It’s been open-sourced under the BSD license, so anyone can use it for free.

Replication constraints

Firstly, binary logs residing on a master must be row-based (STATEMENT and MIXED options won’t do, only ROW). Secondly, this replicator is not a Tarantool module, but a stand-alone daemon that doesn’t have anything to do
================================================================================
Rank = 95; Score = 3407872.0
<|begin_of_text|>I had a great time at React Europe Conf 2016 and thought I would share some of my opinions and things that stood out from the conference.

GraphQL vs and Falcor

I’ve heard of both these frameworks for retrieving data from the server but I’ve never really understood the benefits by just viewing the documentation. After seeing both these frameworks I can honestly see the advantages of using both. Having the ability to retrieve data from the server without specifying a REST endpoint is amazing, it allows you to scale your data model easily.

The talk by Jafar Husain “Falcor: One Model Everywhere” demonstrated the way Falcor does it really well. He also gives credit to GraphQL saying both frameworks have a place in the ecosystem and picking the best one is subjective to your requirements. My take from his talk is that Falcor is better for smaller applications where the data model is known and that you should use GraphQL where the model may change and need to scale.

Falcor is less prescriptive, has fewer concepts and a smaller file size < 50%. It takes a segment of the model you defined and returns it from the data source. I have a few questions around how this works with a real database because the demo was using a JSON file but it’s something I am going to investigate, now I’ve seen it in action.

The talk by Laney Kuenzel and Lee Byron named “GraphQL Future” was a nice insight into the ways Facebook use GraphQL and what they have planned for it in the future. They talk a little bit about the different experiments they are running with GraphQL and working on the BBC LIVE product I can really see how the directive @live in GraphQL will change the future of real time updates in applications. For their demo’s they also used graphiql which looks really cool for testing out your graphQL queries.

Both talks were really well executed and two of the best talks at React Europe Conf 2016.

Carte Blanche

The Evolution of React UI Development by Max Stoiber and Nik Graf stole the show for me. With the announcement of carte-blanche, a tool to help you see components individually, explore them in different states, and quickly develop them confidently will really change my development workflow. Honestly check this out, having a dependency tree with loads of components is a pain when you want to quickly develop, it may mean you have to npm link your dependency tree but with carte-blanche that’s no longer necessary.

A Deep dive into things…

Oh my god, these talks! The subjects
================================================================================
Rank = 96; Score = 3391488.0
<|begin_of_text|>Reseeding is a process of updating the failed mailbox database to be in sync with the Active mailbox database. This has been greatly improved in Exchange 2013. Some of the new and interesting features in Exchange 2013 is Automatic Reseed and multiple databases per volume.

AutoReseed is purposed to overcome the manual efforts of replacing the failed disk with a new disk and restoring the copy of the database on the spare disk ASAP when the disk failure occurs. It involves pre-configuration of the disk and databases using mount point. Auto reseeding has a built in algorithm to perform some basic check before the spare disk is configured. Algorithm uses MS Exchange Replication service to check for the FailedAndSuspended database. Once it finds the FailedAndSuspended DB, it performs some checks, like the number of copies available, disk space, etc. If the checks are successful, then it automatically maps the spare disk and reseeding is performed.

Multiple Databases per volume is the new design improvement when just a bunch of disks (JBOD) configurations is used for the mailbox database. This design allows hosting multiple databases on the same volume.As these days the size of the disks can be increased up to 8 TB, whereas the Exchange best practice guideline only mentions 2 TB, there could be a waste of additional 6 TB. To overcome this issue, Exchange 2013 allows multiple copies per disk. Figure 1 below illustrates the symmetrical design of a database. All four servers have the same four databases all hosted on a single disk per server. The key is that the number of copies of each database should be equal to the number of database copies per disk.

Figure 1. Exchange 2013 multiple databases per volume

When disk failure occurs and disk is reconfigured, then auto reseeding would be kicked from multiple source. This symmetrical design also increases the speed of reseeding from multiple source then the transitional reseeding method of single source. Figure 2 below shows the details of the reseeding from multiple source with the reseeding speed of 2 TB of data in approximately 9.7 hrs.

Figure 2. Improved reseeding using multiple databases per volume

Auto Reseeding and Multiple Databases per volume have greatly improved the seeding of failed database in Exchange 2013. If we do not have the configuration of spare disk and multiple database per volume and there is a database or server failure, then we have to perform the reseeding operation manually. Manual rese
================================================================================
Rank = 97; Score = 3391488.0
<|begin_of_text|>Visualizing TPath Data at Design Time using the LiveBindings Designer

For today's #DelphiWeek blog post, I thought I would highlight the design-time visualization of TPath data.

TPath data represents a series of connected curves and lines. You can use path data to build graphic shapes by connecting a series of curves and lines. You can also use path data to create the path to be followed when applying a TPathAnimation. Several program such as Inkscape allow you to draw your own vector shapes and then export the path data for use in other programs.

This example uses the following components:

TToolbar with a parented TLabel TPath TPrototypeBindSource bound to a TBindNavigator TPath TStyleBook (optional). I loaded the CoralCrystal premium Mac style to it.

Steps:

Place TToolbar onto your form, and parent a TLabel to it. Align TToolbar to the top, align TLabel to Contents, and set HorzAlign to Center. Set TLabel's StyleLookUp property to toollabel.

Place a TPath Component onto your form. Next, place a TMemo onto your form.

In this demo, we are going to use existing TPath sample data. Place a TPrototypeBindSource onto your form, right-click it, select 'Add Field...' and select PathData.

Right-click on TPrototypeBindSource and select 'Add Navigator'.

Place a TMemo onto your form and align it to the bottom. The memo is going to display the actual path data.

Go to View->LiveBindings Designer. Bind PrototypeBindSource1.PathData1 to Path1.Data.Data. Bind Memo1.Text to PrototypeBindSource1.PathData1.

Here is the running application. The BindNavigator component allows you to quickly navigate back and forth between the path data shapes.

Note: You can easily add your own PathData using the Data.Data field for TPath and then preview a single shape using the Path Designer editor:

To learn more about TPath, visit our docwiki.

To learn more about this week's #DelphiWeek celebration, click here.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 98; Score = 3375104.0
<|begin_of_text|>Since its launch just a few years ago, DigitalOcean quickly made a name for itself as a hosting platform for affordable virtual servers. The company’s ambition goes far beyond hosting your WordPress blog or test server on a $5/month machine however and today it is taking a next step in this direction by announcing its support for CoreOS, the highly popular container-centric Linux distribution for massive-scale deployments.

“There is a lot of community excitement for CoreOS,” said Mitch Wainer, co-founder and CMO at DigitalOcean in a statement today. “We’re pleased to announce that developers can begin using CoreOS immediately, ensuring the resiliency of their architecture, as their ability to scale at massive levels increases.”

DigitalOcean tells me that it believes its platform is now the “easiest and least expensive way to try out CoreOS and get started using containers on any cloud service”

Given that running a CoreOS cluster is a bit more intricate than just firing up the latest version of Ubuntu on a single server, DigitalOcean users need to take a few extra steps, too. They need to supply a configuration file when they create a new service (or “droplet” in DigitalOcean’s parlance). CoreOS, after all, needs to know what other servers it can talk to. It’s not all that complex to set up these configuration files, but it’s also not trivial by any means.

With the addition of CoreOS support, DigitalOcean is clearly trying to expand its reach beyond the basic virtual server market. And if it wants to be taken seriously in the competition with AWS and Google Cloud Platform, it does need to offer exactly these kind of services.

As the company’s co-founder Ben Uretsky has often told me, the company was so busy just scaling up to meet demand over the last few years that basic product development like adding more distributions often fell by the wayside. Now that DigitalOcean has plenty of funding and a larger staff, it’s able to focus more on adding new features like today’s CoreOS launch.<|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|><|end_of_text|>
================================================================================
Rank = 99; Score = 3358720.0
<|begin_of_text|>Handling traffic peaks with CloudFlare CDN for free

Every once in a while you're lucky and end up with a positive problem - your website content is suddenly very popular. You might scramble and start turning up your servers and tuning up your caches or maybe someone's de-facto solution is to install HHVM to run your WordPress faster. While this is all worth while if you plan for this to happen in the future as well, for and occasional hit piece of content it might not be worth it.

I've ran a dormant news site, metropolitan.fi, on Bolt since June 2013 for fun. On an average day it barely registers on any meters, averaging maybe 10-20 visitors a day. I run the site on a Virtual Server with multiple other sites on it (including this one).

The publication is by no means a triumph of journalism, it's more of an experiment. And while I don't normally even think about the service I have twice hit a niche and spark discussions by translating some news from Finland that have been noteworthy Globally:

While it's fun to attract some readers, it's not something I want to loose sleep over. The server runs other services and I'd rather not have them be affected by the traffic. The server would probably have managed this just fine without any tweaks, but as a life insurance I set up CloudFlare caching on it back in June 2013.

Promise of the CloudFlare Free Plan is:

Peace of mind about running your website so you can get back to what you love

CloudFlare offers a number of services, but the key take away for me is the advanced Content Delivery Network (CDN). This let's me easily scale up for load peaks when I need - actually even when I know don't need since the service is always-on.

The CDN platform delivers content with speed and closest to where the users is - with low latency. This results in lower load on my server and a better experience for a global audience. In addition to delivery, CloudFlare also offers other optimizations (resource minification, etc.). I've ran with default settings and it works just fine for such a simple content driven site.

So in the title your read that it's free. So what's the catch? Well, you'll run out of the free plan features at some point and then you'll want more. That's when you start paying. Just like with Slack and many other SaaS platforms. It's "freemium" and it works for my use